[
  {
    "objectID": "index.html#시작하기-전에",
    "href": "index.html#시작하기-전에",
    "title": "딥러닝 용어 사전",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n용어 사전은 여러분의 도움이 필요합니다. 빠르게 발전하고 있는 딥러닝 분야는 새로운 용어가 많아 정확한 의미를 찾기 어렵고 한국어 문서를 찾기 어렵습니다. 기계 번역으로 생성되는 한글 문서는 편리하긴 하지만 통일되지 않은 한국어 용어가 사용되는 문제점이 있어 실제 원문에서 전달하려는 의미가 다르게 전달될 수 있습니다.\n만약 각자가 공부했던 내용을 조금씩 정리하며 모으면서 함께 발전하는 용어 사전이 있다면 어떨까요? 아직 용어 사전은 부족해서 찾으려는 한국어 용어와 설명이 없을 수 있지만 참여해 주시는 분들이 늘어날수록 내가 필요한 용어를 쉽게 찾을 수 있을 겁니다.\n시작은 가볍게 궁금했던 용어를 찾아보는 것으로 시작해 보셔도 좋습니다. 도움이 되셨다면 딥러닝을 공부할 때 어려웠던 기억을 떠올리며 사람들에게 여러분의 지식을 나누어 주세요. 용어 사전을 통해서 함께 발전하길 바랍니다."
  },
  {
    "objectID": "index.html#번역-프로젝트",
    "href": "index.html#번역-프로젝트",
    "title": "딥러닝 용어 사전",
    "section": "번역 프로젝트",
    "text": "번역 프로젝트\n용어 사전은 아래의 번역 프로젝트와 함께하고 있습니다. 번역 작업 중인 문서에 대해 프로젝트 카드를 등록을 요청할 수 있으니 용어 사전과 함께 번역 작업을 진행하는 건 어떨까요?\n\n\n\n\n\n\n\n🤗 HuggingFace - Transformers [Link »]\n\n\n\n\n\n\n\n\n\n🤗 HuggingFace - Audio Course [Link »]\n\n\n\n\n\n\n\n\n\n🤗 HuggingFace - Diffuser [Link »]\n\n\n\n\n\n\n\n\n\n함께 해요 - 새로운 프로젝트를 기다립니다."
  },
  {
    "objectID": "dictionary/terms/checkpoint.html",
    "href": "dictionary/terms/checkpoint.html",
    "title": "checkpoint",
    "section": "",
    "text": "딥러닝에서 “checkpoint”란 모델의 중간 상태나 가중치(weight) 및 다른 학습 관련 정보를 저장하는 용어입니다. 이것은 모델 학습 중에 모델의 파라미터 및 학습 과정을 백업하고 모델을 재사용하거나 학습을 다시 시작할 때 사용됩니다. “checkpoint”는 다음과 같은 의미와 목적을 가지고 있습니다:\n모델 복원 및 재사용: 모델 학습 중에 정기적으로 checkpoint를 저장하면 모델의 중간 상태를 나중에 복원하고 재사용할 수 있습니다. 이것은 학습 프로세스를 중단하고 나중에 중단된 곳에서 다시 시작할 때 유용합니다. 예를 들어, 모델 학습이 오랜 시간이 걸리는 경우, 학습이 중간에 실패하거나 중단된 경우, checkpoint를 사용하여 마지막으로 저장된 상태에서 학습을 재개할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/checkpoint.html#참조",
    "href": "dictionary/terms/checkpoint.html#참조",
    "title": "checkpoint",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [딥러닝 checkpoint의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/half_precision.html",
    "href": "dictionary/terms/half_precision.html",
    "title": "half precision",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/tokenizer.html",
    "href": "dictionary/terms/tokenizer.html",
    "title": "tokenizer",
    "section": "",
    "text": "토크나이저는 자연어 처리(Natural Language Processing, NLP) 분야에서 텍스트를 작은 단위로 나누는 도구 또는 프로세스를 가리킵니다. 이 작은 단위는 토큰(Token)이라고 불리며, 일반적으로 단어, 문장 부호, 혹은 하나의 글자와 같은 작은 텍스트 조각을 말합니다.\n토크나이저의 주요 목적은 텍스트를 기계 학습 알고리즘, 딥러닝 모델 또는 다른 자연어 처리 작업에 입력으로 사용할 수 있는 형식으로 변환하는 것입니다. 이렇게 텍스트를 토큰으로 분할하면 기계는 더 쉽게 텍스트를 이해하고 처리할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/tokenizer.html#참조",
    "href": "dictionary/terms/tokenizer.html#참조",
    "title": "tokenizer",
    "section": "참조",
    "text": "참조\n\n[tokenizer]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/submodule.html",
    "href": "dictionary/terms/submodule.html",
    "title": "submodule",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/section.html",
    "href": "dictionary/terms/section.html",
    "title": "section",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/license.html",
    "href": "dictionary/terms/license.html",
    "title": "license",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/pre_train.html",
    "href": "dictionary/terms/pre_train.html",
    "title": "pre-train",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/training.html",
    "href": "dictionary/terms/training.html",
    "title": "training",
    "section": "",
    "text": "딥러닝에서 훈련(training)은 모델을 데이터에 훈련시키는 과정을 의미합니다. 이 과정은 모델이 데이터에서 패턴을 이해하고 문제를 해결하기 위한 모델을 조정하는 것을 포함합니다.\n훈련은 입력 데이터를 모델에 주입하고, 모델은 예측을 수행한 후 손실 함수를 사용하여 오차를 계산하고 역전파(backpropagation)를 통해 파라미터를 업데이트합니다. 이 과정은 여러 에폭(epochs) 동안 반복됩니다."
  },
  {
    "objectID": "dictionary/terms/training.html#참조",
    "href": "dictionary/terms/training.html#참조",
    "title": "training",
    "section": "참조",
    "text": "참조\n\n[딥러닝 training의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/code_snippet.html",
    "href": "dictionary/terms/code_snippet.html",
    "title": "code snippet",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/directory_1.html",
    "href": "dictionary/terms/directory_1.html",
    "title": "directory",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/xlm-roberta.html",
    "href": "dictionary/terms/xlm-roberta.html",
    "title": "XLM-RoBERTa",
    "section": "",
    "text": "XLM-RoBERTa 모델은 Unsupervised Cross-lingual Representation Learning at Scale에서 제안 되었습니다 . 2019년에 출시된 Facebook의 RoBERTa 모델을 기반으로 합니다. 2.5TB의 필터링된 CommonCrawl 데이터에서 훈련된 대규모 다국어 언어 모델입니다.\nRoBERTa 모델은 RoBERTa: A Robustly Optimized BERT Pretraining Approach에서 제안되었습니다. 2018년에 출시된 Google의 BERT 모델을 기반으로 합니다. BERT를 기반으로 하며 주요 하이퍼파라미터를 수정하여 다음 문장 사전 훈련 목표를 제거하고 훨씬 더 큰 미니 배치 및 학습 속도로 훈련합니다."
  },
  {
    "objectID": "dictionary/terms/xlm-roberta.html#section",
    "href": "dictionary/terms/xlm-roberta.html#section",
    "title": "XLM-RoBERTa",
    "section": "",
    "text": "XLM-RoBERTa 모델은 Unsupervised Cross-lingual Representation Learning at Scale에서 제안 되었습니다 . 2019년에 출시된 Facebook의 RoBERTa 모델을 기반으로 합니다. 2.5TB의 필터링된 CommonCrawl 데이터에서 훈련된 대규모 다국어 언어 모델입니다.\nRoBERTa 모델은 RoBERTa: A Robustly Optimized BERT Pretraining Approach에서 제안되었습니다. 2018년에 출시된 Google의 BERT 모델을 기반으로 합니다. BERT를 기반으로 하며 주요 하이퍼파라미터를 수정하여 다음 문장 사전 훈련 목표를 제거하고 훨씬 더 큰 미니 배치 및 학습 속도로 훈련합니다."
  },
  {
    "objectID": "dictionary/terms/xlm-roberta.html#참조",
    "href": "dictionary/terms/xlm-roberta.html#참조",
    "title": "XLM-RoBERTa",
    "section": "참조",
    "text": "참조\n\nhttps://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment\nhttps://github.com/facebookresearch/fairseq/tree/main/examples/xlmr"
  },
  {
    "objectID": "dictionary/terms/clustering.html",
    "href": "dictionary/terms/clustering.html",
    "title": "clustering",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/seperator.html",
    "href": "dictionary/terms/seperator.html",
    "title": "seperator",
    "section": "",
    "text": "딥러닝에서 분할 토큰은 주로 언어 모델(예: GPT, BERT)을 사용하는 자연어 처리(NLP) 작업에서 텍스트를 토큰(token)으로 분할하고, 분할된 토큰 중 하나가 텍스트의 끝이나 다른 문맥적 경계를 나타내는 데 사용되는 특수한 토큰을 가리킵니다.\n일반적으로 언어 모델은 입력 텍스트를 토큰 단위로 분할하고, 각 토큰에 대한 임베딩 벡터를 생성하여 텍스트를 처리합니다. 이때 텍스트의 문맥을 이해하기 위해 각 문장이나 문단의 끝을 나타내는 특별한 토큰이 필요할 수 있습니다. 이 특별한 토큰은 문장 또는 문단 간의 경계를 표시하고 모델에게 언어의 문맥을 알리는 데 사용됩니다.\n예를 들어, 문장 “나는 고양이를 좋아해.”와 “고양이는 너무 귀여워.”가 있다고 가정해 봅시다. 이 두 문장을 토큰화하면 다음과 같을 수 있습니다:\n문장 1: [“나”, “는”, “고양이”, “를”, “좋아해”, “.”] 문장 2: [“고양이”, “는”, “너무”, “귀여워”, “.”] 여기서 마침표(“.”)는 각 문장의 끝을 나타내는 특수한 토큰으로 분할 토큰입니다. 이를 통해 모델은 두 문장이 서로 다른 문장임을 이해하고, 문장 간의 관계를 파악할 수 있습니다.\n또한, 분할 토큰은 대화형 언어 처리 작업에서 각 대화 발화를 구분하는 데도 사용될 수 있습니다. 대화 형식에서는 대개 대화 참가자의 발화 간에 경계를 나타내기 위해 분할 토큰을 사용합니다."
  },
  {
    "objectID": "dictionary/terms/seperator.html#참조",
    "href": "dictionary/terms/seperator.html#참조",
    "title": "seperator",
    "section": "참조",
    "text": "참조\n\n[딥러닝 분할토큰의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/hyperparameter.html",
    "href": "dictionary/terms/hyperparameter.html",
    "title": "hyperparameter",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/lesson.html",
    "href": "dictionary/terms/lesson.html",
    "title": "lesson",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/directory.html",
    "href": "dictionary/terms/directory.html",
    "title": "directory",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/resume.html",
    "href": "dictionary/terms/resume.html",
    "title": "resume",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/learning.html",
    "href": "dictionary/terms/learning.html",
    "title": "learning",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/convolution_1.html",
    "href": "dictionary/terms/convolution_1.html",
    "title": "convolution",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/speech_enhancement.html",
    "href": "dictionary/terms/speech_enhancement.html",
    "title": "speech enhancement",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/casual_language_model.html",
    "href": "dictionary/terms/casual_language_model.html",
    "title": "causal language model",
    "section": "",
    "text": "“Causal language model”은 언어 모델링과 자연어 처리 분야에서 사용되는 모델 중 하나로, 주로 시퀀스 데이터에서 다음 단어나 토큰을 예측하기 위해 사용됩니다. 이 모델은 인과 관계(causal relationship)를 고려하여 시퀀스 데이터를 생성하거나 분석하는 데 중요한 역할을 합니다.\n이러한 이유로 Causal language model은 다양한 응용 분야에서 데이터 분석, 예측, 생성, 이해, 추천 등의 작업을 효과적으로 수행하는 데 활용되며, 자연어 처리 분야에서 매우 중요한 역할을 합니다."
  },
  {
    "objectID": "dictionary/terms/casual_language_model.html#참조",
    "href": "dictionary/terms/casual_language_model.html#참조",
    "title": "causal language model",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [causal language model의 의미, causal language model을 사용하는 이유]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/branch.html",
    "href": "dictionary/terms/branch.html",
    "title": "branch",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/custom.html",
    "href": "dictionary/terms/custom.html",
    "title": "custom",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/inference.html",
    "href": "dictionary/terms/inference.html",
    "title": "inference",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/optimizer.html",
    "href": "dictionary/terms/optimizer.html",
    "title": "optimizer",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/push.html",
    "href": "dictionary/terms/push.html",
    "title": "push",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/step.html",
    "href": "dictionary/terms/step.html",
    "title": "step",
    "section": "",
    "text": "딥러닝에서 “step(스텝)”은 주로 훈련 알고리즘을 실행하거나 모델 파라미터를 업데이트하는 한 단위 작업을 나타냅니다. 데이터의 여러 배치(batch)를 사용하여 학습을 진행하며, 전체 학습 데이터를 여러 번(epoch) 사용하는 것이 일반적입니다. 반복 횟수는 학습의 효과와 모델의 수렴에 영향을 미칩니다."
  },
  {
    "objectID": "dictionary/terms/architecture.html",
    "href": "dictionary/terms/architecture.html",
    "title": "architecture",
    "section": "",
    "text": "아키텍처(architecture)는 다양한 분야에서 사용되는 용어로, 그 의미는 맥락에 따라 다를 수 있습니다. 주로 컴퓨터 과학 및 정보 기술 분야에서 아키텍처란 다음과 같은 의미로 사용됩니다.\n소프트웨어 아키텍처(Software Architecture): 소프트웨어 시스템의 구조와 구성 요소 간의 관계를 설계하는 것을 다룹니다. 소프트웨어 아키텍처는 소프트웨어 시스템의 모듈화, 계층화, 컴포넌트 간의 상호 작용, 데이터 흐름 등을 고려하여 시스템의 확장성, 유지보수성, 재사용성을 향상시키기 위해 사용됩니다."
  },
  {
    "objectID": "dictionary/terms/architecture.html#참조",
    "href": "dictionary/terms/architecture.html#참조",
    "title": "architecture",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [아키텍처의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/method.html",
    "href": "dictionary/terms/method.html",
    "title": "method",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/load.html",
    "href": "dictionary/terms/load.html",
    "title": "load",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/gradient.html",
    "href": "dictionary/terms/gradient.html",
    "title": "gradient",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/autoregressive_model.html",
    "href": "dictionary/terms/autoregressive_model.html",
    "title": "autoregressive model",
    "section": "",
    "text": "자연어 처리와 시계열 데이터 분석과 같은 다양한 분야에서 사용되는 “autoregressive model”은 시계열 데이터나 순차적 데이터를 모델링하기 위한 통계적 또는 머신 러닝 모델의 일종입니다. 이 모델은 현재 시점의 데이터를 이전 시점의 데이터와 관련하여 예측하거나 모델링하는 데 사용됩니다.\nAutoregressive 모델은 시계열 예측, 확률적 시계열 생성, 자연어 처리에서 언어 모델링 등 다양한 응용 분야에서 사용됩니다. 대표적으로 Autoregressive 모델로 알려진 것 중 하나는 자연어 처리에서 사용되는 “언어 모델(Language Model)”입니다. 이 모델은 문장을 생성하거나 다음 단어를 예측하는 데 자주 사용됩니다.\n예를 들어, 자연어 처리에서 GPT-3와 같은 모델은 Autoregressive 모델의 한 유형으로, 이전 단어들을 사용하여 다음 단어를 예측하는 방식으로 텍스트를 생성합니다."
  },
  {
    "objectID": "dictionary/terms/autoregressive_model.html#참조",
    "href": "dictionary/terms/autoregressive_model.html#참조",
    "title": "autoregressive model",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [autoregressive model의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/repository.html",
    "href": "dictionary/terms/repository.html",
    "title": "repository",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/convolution.html",
    "href": "dictionary/terms/convolution.html",
    "title": "convolution",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/mixed_precision.html",
    "href": "dictionary/terms/mixed_precision.html",
    "title": "mixed precision",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/instance.html",
    "href": "dictionary/terms/instance.html",
    "title": "instance",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "contribution.html",
    "href": "contribution.html",
    "title": "기여하신 분",
    "section": "",
    "text": "용어 사전에 기여해주신 모든 분들께 감사드립니다. 새로운 기여자와 팀을 찾습니다. 함께 해주실꺼죠?\n\n\n허깅페이스와 다른 딥러닝 관련 프로젝트를 한글화하고 있습니다. 관련 프로젝트를 번역하는 과정에서 용어를 일관성 있게 번역하기 위해서 사전을 만들고 있습니다.\n\n\n\n이름\n조직\n소개\n\n\n\n\n서원형\nPseudo Lab. Huggingface KREW\n\n\n\n손기훈\nPseudo Lab. Huggingface KREW\n\n\n\n심소현\nPseudo Lab. Huggingface KREW\n\n\n\n양성모\nPseudo Lab. Huggingface KREW\nGithub\n\n\n윤현서\nPseudo Lab. Huggingface KREW\n\n\n\n정우준\nPseudo Lab. Huggingface KREW\nGithub\n\n\n한나연\nPseudo Lab. Huggingface KREW\n\n\n\n\n\n\n\n\n\n\n\n이름\n조직\n소개\n\n\n\n\n이홍규\nPseudo Lab. Diffuser 프로젝트\n\n\n\n박성수\nPseudo Lab. Diffuser 프로젝트\n\n\n\n이준형\nPseudo Lab. Diffuser 프로젝트\n\n\n\n윤형석\nPseudo Lab. Diffuser 프로젝트\n\n\n\n김지환\nPseudo Lab. Diffuser 프로젝트\n\n\n\n안혜민\nPseudo Lab. Diffuser 프로젝트"
  },
  {
    "objectID": "contribution.html#huggingface-krew",
    "href": "contribution.html#huggingface-krew",
    "title": "기여하신 분",
    "section": "",
    "text": "허깅페이스와 다른 딥러닝 관련 프로젝트를 한글화하고 있습니다. 관련 프로젝트를 번역하는 과정에서 용어를 일관성 있게 번역하기 위해서 사전을 만들고 있습니다.\n\n\n\n이름\n조직\n소개\n\n\n\n\n서원형\nPseudo Lab. Huggingface KREW\n\n\n\n손기훈\nPseudo Lab. Huggingface KREW\n\n\n\n심소현\nPseudo Lab. Huggingface KREW\n\n\n\n양성모\nPseudo Lab. Huggingface KREW\nGithub\n\n\n윤현서\nPseudo Lab. Huggingface KREW\n\n\n\n정우준\nPseudo Lab. Huggingface KREW\nGithub\n\n\n한나연\nPseudo Lab. Huggingface KREW"
  },
  {
    "objectID": "contribution.html#허깅페이스-diffuser-번역-프로젝트",
    "href": "contribution.html#허깅페이스-diffuser-번역-프로젝트",
    "title": "기여하신 분",
    "section": "",
    "text": "이름\n조직\n소개\n\n\n\n\n이홍규\nPseudo Lab. Diffuser 프로젝트\n\n\n\n박성수\nPseudo Lab. Diffuser 프로젝트\n\n\n\n이준형\nPseudo Lab. Diffuser 프로젝트\n\n\n\n윤형석\nPseudo Lab. Diffuser 프로젝트\n\n\n\n김지환\nPseudo Lab. Diffuser 프로젝트\n\n\n\n안혜민\nPseudo Lab. Diffuser 프로젝트"
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "가이드 문서",
    "section": "",
    "text": "이 문서는 용어 사전을 사용하는 방법과 용어를 등록하기 위한 정보를 전달하기 위해 작성되었습니다.\n\n\n용어 사전에 등록된 용어 설명 페이지의 구조를 알아봅니다. 용어 사전에 등록된 상세 페이지는 영어와 한국어 용어 이름으로 구성됩니다. 아래의 archirecture로 등록된 상세 페이지를 통해 좀 더 자세히 알아보겠습니다. architecture라는 영어에 대한 한국어 용어명이 바로 아키텍처로 표시됩니다. 바로 밑에는 마크다운 배지로 영어 용어가 사용되었던 원본 문서 링크가 연결되었습니다.\n\narcitrecture용어에 대한 웹페이지는 용어 사전 소스파일의 archirecture.qmd파일의 정보로 생성됩니다. 코드의 상단의 title과 description은 각각 용어의 영어와 한국어 표현을 나타내고 url은 용어가 나온 원본 문서의 위치를 의미합니다. 용어 사전의 정보가 코드로 어떻게 표현되었는지 아래의 코드에서 확인할 수 있습니다.\n---\ntitle: 'architecture'\ndescription: '아키텍처'\nurl: https://huggingface.co/docs/transformers/index\n---\n&lt;a href=\"https://huggingface.co/docs/transformers/index\" target=\"_blank\"&gt;\n    &lt;img loading=\"lazy\"\n     alt=\"src: HuggingFace Transformer\"\n     src=\"https://img.shields.io/badge/문서-Huggingface_Transformer-blue\" &gt;\n&lt;/a&gt;\n딥러닝 분야에 따라 동일한 용어가 다른 뜻으로 사용되거나 문서에 따라 한국어 번역 방식이 다를 수 있습니다. 각 문서에는 원본 문서에 대한 링크가 있기 때문에 원본 문서별로 일관성 있게 번역 용어가 사용될 수 있습니다. 마크다운 배지는 image shields io를 이용합니다. 배지의 생성 방식은 코드에서 이미지 소스의 주소를 잘 살펴보면 이해할 수 있습니다. 마크다운 배지에 표시되는 문구는 img shields io에 전달하는 URL 정보와 동일하네요. 위의 코드에서는 문서-Huggingface_Transformer-blue 를 이미지 위치로 전달했고 문서, Huggingface_Transformer, blue의 3개의 배지 정보는 구분자 -로 분리됩니다. 공백을 _로 표시했음을 주의하세요."
  },
  {
    "objectID": "guide.html#용어-페이지-구조",
    "href": "guide.html#용어-페이지-구조",
    "title": "가이드 문서",
    "section": "",
    "text": "용어 사전에 등록된 용어 설명 페이지의 구조를 알아봅니다. 용어 사전에 등록된 상세 페이지는 영어와 한국어 용어 이름으로 구성됩니다. 아래의 archirecture로 등록된 상세 페이지를 통해 좀 더 자세히 알아보겠습니다. architecture라는 영어에 대한 한국어 용어명이 바로 아키텍처로 표시됩니다. 바로 밑에는 마크다운 배지로 영어 용어가 사용되었던 원본 문서 링크가 연결되었습니다.\n\narcitrecture용어에 대한 웹페이지는 용어 사전 소스파일의 archirecture.qmd파일의 정보로 생성됩니다. 코드의 상단의 title과 description은 각각 용어의 영어와 한국어 표현을 나타내고 url은 용어가 나온 원본 문서의 위치를 의미합니다. 용어 사전의 정보가 코드로 어떻게 표현되었는지 아래의 코드에서 확인할 수 있습니다.\n---\ntitle: 'architecture'\ndescription: '아키텍처'\nurl: https://huggingface.co/docs/transformers/index\n---\n&lt;a href=\"https://huggingface.co/docs/transformers/index\" target=\"_blank\"&gt;\n    &lt;img loading=\"lazy\"\n     alt=\"src: HuggingFace Transformer\"\n     src=\"https://img.shields.io/badge/문서-Huggingface_Transformer-blue\" &gt;\n&lt;/a&gt;\n딥러닝 분야에 따라 동일한 용어가 다른 뜻으로 사용되거나 문서에 따라 한국어 번역 방식이 다를 수 있습니다. 각 문서에는 원본 문서에 대한 링크가 있기 때문에 원본 문서별로 일관성 있게 번역 용어가 사용될 수 있습니다. 마크다운 배지는 image shields io를 이용합니다. 배지의 생성 방식은 코드에서 이미지 소스의 주소를 잘 살펴보면 이해할 수 있습니다. 마크다운 배지에 표시되는 문구는 img shields io에 전달하는 URL 정보와 동일하네요. 위의 코드에서는 문서-Huggingface_Transformer-blue 를 이미지 위치로 전달했고 문서, Huggingface_Transformer, blue의 3개의 배지 정보는 구분자 -로 분리됩니다. 공백을 _로 표시했음을 주의하세요."
  },
  {
    "objectID": "guide.html#준비-하기",
    "href": "guide.html#준비-하기",
    "title": "가이드 문서",
    "section": "준비 하기",
    "text": "준비 하기\n딥러닝 용어 사전은 quarto를 이용하여 작성되었습니다. Quarto 웹사이트로 이동하여 프로그램을 설치해주세요. 설치가 완료되었다면 이제 본격적으로 문서를 작성할 준비를 끝났습니다.\n용어 사전의 구조를 살펴볼까요? 용어 사전에 작성된 용어들은 아래의 terms폴더에 하나의 파일 형태로 생성됩니다. 예를 들어 convolution을 수정하는 경우 terms 폴더에 생성된 convolution.qmd파일을 수정합니다.\n├── glossary\n│   ├── dictionary\n|   │   └── terms\n|   |       ├── architecture.qmd\n|   |               ...\n|   |       ├── convolution.qmd\n|   |               ...\n|   │       └── speech_enhancement.qmd\n\n새로운 용어 작성\n앗, 제가 찾는 용어가 없습니다! 새로운 용어를 등록하는 경우 terms폴더에 등록하려는 용어명과 동일한 파일명으로 qmd파일을 생성합니다.\n\n\n\n\n\n\n신규용어 파일명 생성 규칙\n\n\n\n\n신규 용어는 모두 소문자이며 공백은 _로 변경하여 파일명을 생성합니다.\n파일명은 모두 소문자로 작성하고 공백은 _로 대체하여 사용하고 있습니다.\n같은 파일명이 있는 경우 _1, _2와 같이 postfix를 붙여 추가합니다.\n\ndirectory.qmd\ndirectory_1.qmd\n\n\n\n\n신규 용어 파일명 생성 규칙에 맞게 파일을 생성한 후 문서 작성을 시작합니다."
  },
  {
    "objectID": "guide.html#문서-작성",
    "href": "guide.html#문서-작성",
    "title": "가이드 문서",
    "section": "문서 작성",
    "text": "문서 작성\n딥러닝 용어사전은 깃헙 리포지토리(repository)로 관리되기 때문에 문서 작성을 위해서 Pseudo-Lab 깃 리포지토리를 Fork하여 문서를 작업합니다. Fork된 리포지토리에서 수정할 웹페이지의 html 파일명과 동일한 이름의 qmd 확장자 파일을 열어 문서를 수정할 수 있습니다.\n문서 작성은 기본적으로 markdown언어를 이용하여 작성하지만 markdown에 익숙하지 않은 경우라면 Visual Code를 이용하는 것을 추천드립니다. Visual Code의 extention 메뉴에서 quarto를 검색하여 익스텐션을 설치할 수 있으며 Figure 1 의 quarto 익스텐션은 GUI환경에서 문서를 쉽게 작성있도록 도와줍니다.\n\n\n\nFigure 1: Quarto Extention\n\n\nVisual Code에서 자신이 수정할 문서를 선택하고 마우스 우측 버튼을 누르면 Edit in Visual Mode를 선택할 수 있습니다. 선택을 완료하면 markdown형태로 표시되던 문서가 wysiwyg1 에디터로 변경되고 Figure 2 와 같이 메뉴를 통해서 문서를 작성할 수 있습니다.\n\n\n\nFigure 2: VS Code Visual Mode"
  },
  {
    "objectID": "guide.html#미리-보기",
    "href": "guide.html#미리-보기",
    "title": "가이드 문서",
    "section": "미리 보기",
    "text": "미리 보기\nVS Code등의 에디터를 사용했다면 자신이 수정한 내용을 바로 확인할 수 있습니다. 만일 GUI방식으로 문서를 작성하지 않는 경우 아래의 quarto preview명령을 이용해서 문서를 로컬 웹브라우져에서 확인할 수 있습니다.\ncd glossary\nquarto preview\npreview명령을 수행하면 로컬 컴퓨터에 브라우져가 실행되고 수정된 웹사이트를 바로 확인할 수 있습니다. 변경사항에 문제가 없다면 이제 PR(Pull Request)를 진행할 수 있습니다."
  },
  {
    "objectID": "guide.html#footnotes",
    "href": "guide.html#footnotes",
    "title": "가이드 문서",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwysiwyg(What You See is What You Get)의 약자로 문서 작성 방법을 GUI로 구현한 에디터 입니다.↩︎\nhttps://won.hashnode.dev/a-comprehensive-guide-to-mastering-github-pr-reviews↩︎\nhttp://www.publickrdata.com/blog/74/↩︎"
  },
  {
    "objectID": "dictionary/terms/denoise.html",
    "href": "dictionary/terms/denoise.html",
    "title": "denoise",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/spectrogram.html",
    "href": "dictionary/terms/spectrogram.html",
    "title": "spectrogram",
    "section": "",
    "text": "스펙트로그램(Spectrogram)은 시간과 주파수를 2차원 그래픽 형태로 나타내는 시각화 도구입니다. 스펙트로그램은 주로 오디오 신호, 음악, 음성 등의 시계열 데이터를 분석하고 표현하는 데 사용됩니다.\n스펙트로그램은 시간에 따른 주파수 변화를 시각적으로 보여줍니다. 가로 축은 시간을, 세로 축은 주파수를 나타내며, 플롯의 색상 농도는 해당 시간과 주파수에서의 신호 강도를 나타냅니다."
  },
  {
    "objectID": "dictionary/terms/spectrogram.html#참조",
    "href": "dictionary/terms/spectrogram.html#참조",
    "title": "spectrogram",
    "section": "참조",
    "text": "참조\n\n[spectogram의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/crob.html",
    "href": "dictionary/terms/crob.html",
    "title": "crob",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/entity.html",
    "href": "dictionary/terms/entity.html",
    "title": "entity",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/fine_tunning_1.html",
    "href": "dictionary/terms/fine_tunning_1.html",
    "title": "fine tunning",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/feature_extraction.html",
    "href": "dictionary/terms/feature_extraction.html",
    "title": "feature extraction",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/batch.html",
    "href": "dictionary/terms/batch.html",
    "title": "batch",
    "section": "",
    "text": "“Batch”는 데이터 처리 및 머신 러닝에서 중요한 개념 중 하나이며, 일괄 처리를 의미합니다. 주로 다음과 같은 두 가지 의미로 사용됩니다.\n데이터 배치(Batch of Data):\n데이터 배치란 데이터 집합(예: 이미지, 텍스트, 숫자 등)을 작은 덩어리로 나눈 것을 의미합니다. 각 덩어리를 배치라고 하며, 한 배치에는 여러 데이터 포인트가 포함됩니다. 예를 들어, 이미지 처리에서 한 배치는 여러 장의 이미지로 구성됩니다. 배치를 사용하는 이유는 모델 학습의 효율성을 높이고, GPU 또는 CPU와 같은 하드웨어 가속기를 효율적으로 활용하기 위함입니다. 배치 처리를 통해 여러 데이터 포인트를 동시에 처리하면 병렬 처리가 가능하며 학습 속도를 향상시킬 수 있습니다. 일괄 처리( Batch Processing):\n일괄 처리는 컴퓨터 과학 및 데이터 처리 분야에서 사용되는 용어로, 데이터를 일정 크기의 묶음(배치)으로 처리하는 방식을 의미합니다. 대규모 데이터 처리 작업을 보다 효율적으로 수행하고, 데이터베이스에서 쿼리를 실행하거나 배치 작업을 수행하는 등의 다양한 컴퓨팅 작업에 사용됩니다. 예를 들어, 머신 러닝에서 데이터를 학습할 때, 훈련 데이터 세트를 작은 배치로 나누어 각 배치를 모델에 공급하여 학습합니다. 이렇게 하면 모델이 전체 데이터 세트를 한 번에 처리하지 않고, 배치 단위로 처리하므로 학습 과정이 효율적으로 이루어집니다.\n데이터를 배치로 나누는 방식은 머신 러닝 모델 및 작업에 따라 다를 수 있으며, 배치 크기(batch size)는 모델의 성능과 학습 속도에 영향을 미칠 수 있습니다. 적절한 배치 크기를 선택하는 것은 모델 학습 및 성능 조정에서 중요한 요소 중 하나입니다."
  },
  {
    "objectID": "dictionary/terms/batch.html#참조",
    "href": "dictionary/terms/batch.html#참조",
    "title": "batch",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [batch의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/prompt.html",
    "href": "dictionary/terms/prompt.html",
    "title": "prompt",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/method_1.html",
    "href": "dictionary/terms/method_1.html",
    "title": "model",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/silent_error.html",
    "href": "dictionary/terms/silent_error.html",
    "title": "silent error",
    "section": "",
    "text": "조용한 오류(silent error)는 프로그램 또는 코드 실행 중에 발생하는 오류 중에서 명시적인 오류 메시지나 경고 없이 발생하며, 프로그램이 비정상적으로 동작하거나 예상치 못한 결과를 내는 오류를 가리킵니다. 이러한 오류는 디버깅하기 어려우며, 프로그램의 안정성과 신뢰성에 영향을 미칠 수 있습니다.\n조용한 오류가 발생하는 경우, 프로그램은 오류가 있음에도 불구하고 실행을 계속하며, 이로 인해 예기치 않은 결과를 생성할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/silent_error.html#참조",
    "href": "dictionary/terms/silent_error.html#참조",
    "title": "silent error",
    "section": "참조",
    "text": "참조\n\n[silent error의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/hidden_state.html",
    "href": "dictionary/terms/hidden_state.html",
    "title": "hidden state",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/dataset.html",
    "href": "dictionary/terms/dataset.html",
    "title": "dataset",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/attention_mask.html",
    "href": "dictionary/terms/attention_mask.html",
    "title": "attention mask",
    "section": "",
    "text": "“Attention mask”는 주로 자연어 처리와 관련된 딥 러닝 모델에서 사용되는 개념입니다. 이것은 주로 트랜스포머(Transformer) 아키텍처와 관련이 있으며, 특히 어텐션 메커니즘(attention mechanism)을 사용하는 모델에서 주로 나타납니다.\n어텐션 메커니즘은 입력 시퀀스의 각 요소에 가중치를 할당하여 중요한 부분을 강조하고, 모델이 해당 부분에 집중하도록 돕는 기술입니다. 이 때, 어텐션 가중치(attention weights)를 결정하는 데 사용되는 정보가 “attention mask”입니다."
  },
  {
    "objectID": "dictionary/terms/attention_mask.html#참조",
    "href": "dictionary/terms/attention_mask.html#참조",
    "title": "attention mask",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [attention mask의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/sequence.html",
    "href": "dictionary/terms/sequence.html",
    "title": "sequence",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/token.html",
    "href": "dictionary/terms/token.html",
    "title": "token",
    "section": "",
    "text": "딥러닝에서 “token(토큰)”은 텍스트 데이터를 작은 단위로 분할하는 과정에서 나오는 기본적인 단위를 나타냅니다. 이 단위는 일반적으로 단어, 부분 단어, 문자 또는 하위 단어 단위일 수 있습니다. 토큰화(tokenization)는 텍스트 데이터를 이러한 토큰으로 분해하는 과정을 말합니다."
  },
  {
    "objectID": "dictionary/terms/token.html#참조",
    "href": "dictionary/terms/token.html#참조",
    "title": "token",
    "section": "참조",
    "text": "참조\n\n[딥러닝 token의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/vocoder.html",
    "href": "dictionary/terms/vocoder.html",
    "title": "vocoder",
    "section": "",
    "text": "오디오를 생성하는 모델은 로그 멜 스펙트로그램을 출력으로 생성하는 것이 일반적입니다. 사람이 인지할 수 있는 파형으로 변경하는 신경망인 보코더를 사용합니다. Bark와 같은 오디오 딥러닝 모델은 원시 음성 파형을 직접 생성합니다. 이와 같은 모델들은 별도의 보코더가 필요하지 않습니다."
  },
  {
    "objectID": "dictionary/terms/vocoder.html#참조",
    "href": "dictionary/terms/vocoder.html#참조",
    "title": "vocoder",
    "section": "참조",
    "text": "참조\n\nhttps://huggingface.co/learn/audio-course/en/chapter6/pre-trained_models"
  },
  {
    "objectID": "dictionary/terms/data_collator.html",
    "href": "dictionary/terms/data_collator.html",
    "title": "data collator",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/chunk.html",
    "href": "dictionary/terms/chunk.html",
    "title": "chunk",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/automatic_speech_recognition.html",
    "href": "dictionary/terms/automatic_speech_recognition.html",
    "title": "automatic speech recognition",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/reproducibility.html",
    "href": "dictionary/terms/reproducibility.html",
    "title": "reproducibility",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/argumentation.html",
    "href": "dictionary/terms/argumentation.html",
    "title": "argumentation",
    "section": "",
    "text": "딥러닝 영상처리 모델의 데이터 증강(augmentation)은 훈련 데이터를 다양하게 변형시켜 모델의 성능을 향상시키는 기법을 말합니다. 데이터 증강은 원본 훈련 데이터를 변형하여 새로운 데이터를 생성하고 이를 훈련에 활용하는 것입니다. 이로 인해 모델은 더 다양한 상황과 조건에서 일반화하고, 과적합을 줄이는데 도움이 됩니다.\n딥러닝 모델은 많은 훈련 데이터를 필요로 하기 때문에, 특히 영상처리 분야에서는 많은 양의 라벨링된 데이터를 구하기 어려울 수 있습니다. 데이터 증강은 이러한 한계를 극복하고, 작은 데이터셋으로도 더 효과적인 훈련이 가능하도록 돕습니다.\n영상처리 분야에서 흔히 사용되는 데이터 증강 방법들은 다음과 같습니다:\n데이터 증강은 더 다양한 데이터를 확보하고, 모델의 일반화 능력을 향상시켜 영상처리 애플리케이션에서 더 좋은 성능을 달성하는데 도움이 됩니다."
  },
  {
    "objectID": "dictionary/terms/argumentation.html#참조",
    "href": "dictionary/terms/argumentation.html#참조",
    "title": "argumentation",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 8월 4일). OpenAI. [딥러닝 영상처리 모델, augumentation 의미]. https://chat.openai.com/\nAPA 출처 생성기 (2023, 8월 4일). ESSAYREVIEW. https://essayreview.co.kr/citation-generator"
  },
  {
    "objectID": "dictionary/terms/multi-image_batches.html",
    "href": "dictionary/terms/multi-image_batches.html",
    "title": "multi-image batches",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/inpainting.html",
    "href": "dictionary/terms/inpainting.html",
    "title": "inpainting",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/arguments.html",
    "href": "dictionary/terms/arguments.html",
    "title": "arguments",
    "section": "",
    "text": "“arguments”는 컴퓨터 프로그래밍에서 사용되는 용어로, 함수 내에서 전달된 인자(인수)에 접근하거나 관리하기 위해 사용됩니다. 다양한 프로그래밍 언어에서 이 용어를 사용하지만, 그 구체적인 사용 방법은 언어에 따라 다를 수 있습니다.\n일반적으로 “arguments”는 함수 내에서 다음과 같은 목적으로 사용됩니다:\n인수(Arguments)에 접근: 함수가 호출될 때 전달된 모든 인수(매개변수)에 접근할 수 있도록 합니다. 이를 통해 함수 내에서 인수의 값을 읽을 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/arguments.html#참조",
    "href": "dictionary/terms/arguments.html#참조",
    "title": "arguments",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 8월 4일). OpenAI. [arguments의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/evaluation_method.html",
    "href": "dictionary/terms/evaluation_method.html",
    "title": "evaluation method",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/dimensions_ordering.html",
    "href": "dictionary/terms/dimensions_ordering.html",
    "title": "dimensions ordering",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/down_stream.html",
    "href": "dictionary/terms/down_stream.html",
    "title": "down stream",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/fine_tunning.html",
    "href": "dictionary/terms/fine_tunning.html",
    "title": "fine tunning",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/custom_1.html",
    "href": "dictionary/terms/custom_1.html",
    "title": "custom",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/pad.html",
    "href": "dictionary/terms/pad.html",
    "title": "parameter",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/epoch.html",
    "href": "dictionary/terms/epoch.html",
    "title": "epoch",
    "section": "",
    "text": "딥러닝에서 에폭은 학습 데이터 전체를 한 번 순회하는 것을 의미합니다. 딥러닝 모델의 가중치는 데이터 묶음인 배치를 통해 업데이트 됩니다. 따라서 배치 크기에 따라서 1 에폭에 훈련되는 횟수가 달라집니다. 예를 들어 전체 100개의 데이터를 10개의 샘플을 갖는 배치 사이즈로 훈련하는 경우 1 에폭에 10회의 훈련이 진행됩니다."
  },
  {
    "objectID": "dictionary/terms/epoch.html#참조",
    "href": "dictionary/terms/epoch.html#참조",
    "title": "epoch",
    "section": "참조",
    "text": "참조\n\nhttps://machinelearningmastery.com/difference-between-a-batch-and-an-epoch"
  },
  {
    "objectID": "dictionary/terms/distributed_training.html",
    "href": "dictionary/terms/distributed_training.html",
    "title": "distributed training",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/prior-preserving.html",
    "href": "dictionary/terms/prior-preserving.html",
    "title": "prior-preserving",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/feature_matrix.html",
    "href": "dictionary/terms/feature_matrix.html",
    "title": "feature matrix",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/backward.html",
    "href": "dictionary/terms/backward.html",
    "title": "backward",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/glossary.html",
    "href": "dictionary/glossary.html",
    "title": "용어 사전",
    "section": "",
    "text": "딥러닝관련 프로젝트를 번역하는 과정에서 용어를 일관성 있게 번역하기 위해서 사전을 만들고 있습니다. 딥러닝 문서 번역에 관심있는 분들 모두 참가가 가능하니 관심있는 분들의 많은 참여 부탁드립니다.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nXLM-RoBERTa\n\n\n\n\n\nXLM-RoBERTa\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/model_doc/xlm-roberta\n\n\n\n\n\n\n\n\narchitecture\n\n\n\n\n\n아키텍처\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nargumentation\n\n\n\n\n\n증강\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\narguments\n\n\n\n\n\n인수\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nattention mask\n\n\n\n\n\n어텐션 마스크\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nautomatic speech recognition\n\n\n\n\n\n자동음성인식\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nautoregressive model\n\n\n\n\n\n자기회귀 모델\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nbackward\n\n\n\n\n\n역방향\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nbatch\n\n\n\n\n\n배치\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nbranch\n\n\n\n\n\n브랜치\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ncausal language model\n\n\n\n\n\n인과적 언어 모델링\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncheckpoint\n\n\n\n\n\n체크포인트\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nchunk\n\n\n\n\n\n묶음\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nclustering\n\n\n\n\n\n군집화\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncode snippet\n\n\n\n\n\n예시 코드\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nconvolution\n\n\n\n\n\n합성곱\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nconvolution\n\n\n\n\n\n컨볼루션\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ncrob\n\n\n\n\n\n자르기\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncustom\n\n\n\n\n\n사용자정의\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncustom\n\n\n\n\n\n커스텀\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndata collator\n\n\n\n\n\n데이터 콜레이터\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndataset\n\n\n\n\n\n데이터 세트\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndenoise\n\n\n\n\n\n디노이즈\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndimensions ordering\n\n\n\n\n\n차원순서(dimensions ordering)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndirectory\n\n\n\n\n\n디렉터리\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndirectory\n\n\n\n\n\n디렉토리\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndistributed training\n\n\n\n\n\n분산 학습\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndown stream\n\n\n\n\n\n다운 스트림\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nentity\n\n\n\n\n\n개체\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nepoch\n\n\n\n\n\n에폭\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nevaluation method\n\n\n\n\n\n평가 방법\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfeature extraction\n\n\n\n\n\n특성 추출\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfeature matrix\n\n\n\n\n\n특성 행렬\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfine tunning\n\n\n\n\n\n파인튜닝\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfine tunning\n\n\n\n\n\n미세 조정\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ngradient\n\n\n\n\n\n그래디언트\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nhalf precision\n\n\n\n\n\n반정밀도(half precision)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nhidden state\n\n\n\n\n\n은닉 상태\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nhyperparameter\n\n\n\n\n\n하이퍼파라미터\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ninference\n\n\n\n\n\n추론\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ninpainting\n\n\n\n\n\n인페인팅\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ninstance\n\n\n\n\n\n인스턴스\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n학습\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nlesson\n\n\n\n\n\n단원\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nlicense\n\n\n\n\n\n라이선스\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nload\n\n\n\n\n\n가져오다\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nmethod\n\n\n\n\n\n매소드\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nmixed precision\n\n\n\n\n\n혼합 정밀도(mixed precision)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nmodel\n\n\n\n\n\n모델\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nmulti-image batches\n\n\n\n\n\n다중 이미지 배치(multi-image batches)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\noptimizer\n\n\n\n\n\n옵티마이저\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nparameter\n\n\n\n\n\n매개변수\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\npre-train\n\n\n\n\n\n사전학습\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nprior-preserving\n\n\n\n\n\n사전 보존(prior-preserving)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nprompt\n\n\n\n\n\n프롬프트\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\npush\n\n\n\n\n\n푸시\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nrepository\n\n\n\n\n\n리포지토리\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nreproducibility\n\n\n\n\n\n재현성\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nresume\n\n\n\n\n\n재개\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nsection\n\n\n\n\n\n섹션\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nseperator\n\n\n\n\n\n분할 토큰\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nsequence\n\n\n\n\n\n시퀀스\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nsilent error\n\n\n\n\n\n조용한 오류\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nspectrogram\n\n\n\n\n\n스펙토그램\n\n\n\n\n\n\nhttps://huggingface.co/learn/audio-course/chapter0/introduction\n\n\n\n\n\n\n\n\nspeech enhancement\n\n\n\n\n\n음성 향상\n\n\n\n\n\n\nhttps://huggingface.co/learn/audio-course/\n\n\n\n\n\n\n\n\nstep\n\n\n\n\n\n스텝\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nsubmodule\n\n\n\n\n\n서브모듈\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ntoken\n\n\n\n\n\n토큰\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ntokenizer\n\n\n\n\n\n토크나이저\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ntraining\n\n\n\n\n\n훈련\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nvocoder\n\n\n\n\n\n보코더\n\n\n\n\n\n\nhuggingface_audio\n\n\nhttps://huggingface.co/learn/audio-course/chapter0/introduction\n\n\n\n\n\n\nNo matching items"
  }
]