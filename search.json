[
  {
    "objectID": "index.html#시작하기-전에",
    "href": "index.html#시작하기-전에",
    "title": "딥러닝 용어 사전",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n용어 사전은 여러분의 도움이 필요합니다. 빠르게 발전하고 있는 딥러닝 분야는 새로운 용어가 많아 정확한 의미를 찾기 어렵고 한국어 문서를 찾기 어렵습니다. 기계 번역으로 생성되는 한글 문서는 편리하긴 하지만 통일되지 않은 한국어 용어가 사용되는 문제점이 있어 실제 원문에서 전달하려는 의미가 다르게 전달될 수 있습니다.\n만약 각자가 공부했던 내용을 조금씩 정리하며 모으면서 함께 발전하는 용어 사전이 있다면 어떨까요? 아직 용어 사전은 부족해서 찾으려는 한국어 용어와 설명이 없을 수 있지만 참여해 주시는 분들이 늘어날수록 내가 필요한 용어를 쉽게 찾을 수 있을 겁니다.\n시작은 가볍게 궁금했던 용어를 찾아보는 것으로 시작해 보셔도 좋습니다. 도움이 되셨다면 딥러닝을 공부할 때 어려웠던 기억을 떠올리며 사람들에게 여러분의 지식을 나누어 주세요. 용어 사전을 통해서 함께 발전하길 바랍니다."
  },
  {
    "objectID": "index.html#번역-프로젝트",
    "href": "index.html#번역-프로젝트",
    "title": "딥러닝 용어 사전",
    "section": "번역 프로젝트",
    "text": "번역 프로젝트\n용어 사전은 아래의 번역 프로젝트와 함께하고 있습니다. 번역 작업 중인 문서에 대해 프로젝트 카드를 등록을 요청할 수 있으니 용어 사전과 함께 번역 작업을 진행하는 건 어떨까요?\n\n\n\n\n\n\n\n🤗 HuggingFace - Transformers [Link »]\n\n\n\n\n\n\n\n\n\n🤗 HuggingFace - Audio Course [Link »]\n\n\n\n\n\n\n\n\n\n🤗 HuggingFace - Diffuser [Link »]\n\n\n\n\n\n\n\n\n\n함께 해요 - 새로운 프로젝트를 기다립니다."
  },
  {
    "objectID": "dictionary/terms/checkpoint.html",
    "href": "dictionary/terms/checkpoint.html",
    "title": "checkpoint",
    "section": "",
    "text": "딥러닝에서 체크포인트란 모델의 중간 상태나 가중치(weight) 및 다른 학습 관련 정보를 저장하는 용어입니다.\n이것은 모델 학습 중에 모델의 파라미터 및 학습 과정을 백업하고 모델을 재사용하거나 학습을 다시 시작할 때 사용됩니다.\n모델 학습 중에 정기적으로 체크포인트를 저장하면 모델의 중간 상태를 나중에 복원하고 재사용할 수 있습니다. 이것은 학습 프로세스를 중단하고 나중에 중단된 곳에서 다시 시작할 때 유용합니다.\n예를 들어, 모델 학습이 오랜 시간이 걸리는 경우, 학습이 중간에 실패하거나 중단된 경우, 체크포인트를 사용하여 마지막으로 저장된 상태에서 학습을 재개할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/checkpoint.html#참조",
    "href": "dictionary/terms/checkpoint.html#참조",
    "title": "checkpoint",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [딥러닝 checkpoint의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/backward.html",
    "href": "dictionary/terms/backward.html",
    "title": "backward",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/feature_matrix.html",
    "href": "dictionary/terms/feature_matrix.html",
    "title": "feature matrix",
    "section": "",
    "text": "딥러닝에서 특성 행렬은 모델의 입력 데이터를 나타내는 행렬입니다. 이 행렬은 각 행이 데이터 샘플을 나타내고 각 열이 특징(또는 특성)을 나타냅니다.\n각 데이터 샘플은 이러한 특징의 값을 포함하며, 딥러닝 모델은 이러한 값들을 기반으로 예측, 분류 또는 다른 작업을 수행합니다."
  },
  {
    "objectID": "dictionary/terms/feature_matrix.html#참조",
    "href": "dictionary/terms/feature_matrix.html#참조",
    "title": "feature matrix",
    "section": "참조",
    "text": "참조\n\n[feature matrix의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/prior-preserving.html",
    "href": "dictionary/terms/prior-preserving.html",
    "title": "prior-preserving",
    "section": "",
    "text": "사전 보전은 딥러닝 또는 확률 모델에서 사용되는 개념 중 하나입니다. 이 용어는 모델의 훈련 과정 중에 발생하는 변화나 업데이트가 모델의 사전 지식(prior knowledge)을 보존하는 것을 나타냅니다.\n모델을 훈련할 때, 모델은 입력 데이터와 목표 출력 데이터 간의 관계를 학습합니다. 그러나 때로는 모델에 사전 지식을 주입하려고 합니다. 사전 지식은 모델이 이미 알고 있는 정보나 특정 제약 조건을 나타낼 수 있습니다. 사전 보존은 모델의 훈련 과정에서 이러한 사전 지식을 보존하려는 의도를 나타냅니다.\n\n참조\n\n[prior-preserving의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/distributed_training.html",
    "href": "dictionary/terms/distributed_training.html",
    "title": "distributed training",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/point_wise_convolution.html",
    "href": "dictionary/terms/point_wise_convolution.html",
    "title": "point-wise convolution",
    "section": "",
    "text": "채널 정보 믹싱"
  },
  {
    "objectID": "dictionary/terms/point_wise_convolution.html#참조",
    "href": "dictionary/terms/point_wise_convolution.html#참조",
    "title": "point-wise convolution",
    "section": "참조",
    "text": "참조\n\n\nhttps://nn.labml.ai/conv_mixer/index.html"
  },
  {
    "objectID": "dictionary/terms/license.html",
    "href": "dictionary/terms/license.html",
    "title": "license",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/validation.html",
    "href": "dictionary/terms/validation.html",
    "title": "validation",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/validation.html#참조",
    "href": "dictionary/terms/validation.html#참조",
    "title": "validation",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/custom_1.html",
    "href": "dictionary/terms/custom_1.html",
    "title": "custom",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/fine_tunning.html",
    "href": "dictionary/terms/fine_tunning.html",
    "title": "fine tunning",
    "section": "",
    "text": "딥러닝에서 미세 조정은 미리 훈련된(pre-trained) 모델을 가져와서 추가적인 훈련을 수행하는 과정을 가리킵니다.\n이것은 주로 전이 학습(transfer learning)의 한 형태로 사용되며, 이미 훈련된 모델의 일부 레이어나 파라미터를 조정하여 새로운 작업에 맞게 모델을 개선하는데 사용됩니다."
  },
  {
    "objectID": "dictionary/terms/fine_tunning.html#참조",
    "href": "dictionary/terms/fine_tunning.html#참조",
    "title": "fine tunning",
    "section": "참조",
    "text": "참조\n\n[fine tunning의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/down_stream.html",
    "href": "dictionary/terms/down_stream.html",
    "title": "down stream",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/dimensions_ordering.html",
    "href": "dictionary/terms/dimensions_ordering.html",
    "title": "dimensions ordering",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/evaluation_method.html",
    "href": "dictionary/terms/evaluation_method.html",
    "title": "evaluation method",
    "section": "",
    "text": "딥러닝에서 평가 방법은 모델의 성능을 측정하는 데 사용됩니다. 이것은 모델이 주어진 작업을 얼마나 잘 수행하는지를 나타냅니다. 이를 통해 모델의 예측 정확도, 정밀도, 재현율, F1 점수 등과 같은 성능 지표를 계산할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/evaluation_method.html#참조",
    "href": "dictionary/terms/evaluation_method.html#참조",
    "title": "evaluation method",
    "section": "참조",
    "text": "참조\n\n[evaluation method의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/depth_wise_convolution.html",
    "href": "dictionary/terms/depth_wise_convolution.html",
    "title": "depth-wise convolution",
    "section": "",
    "text": "공간 정보 믹싱"
  },
  {
    "objectID": "dictionary/terms/depth_wise_convolution.html#참조",
    "href": "dictionary/terms/depth_wise_convolution.html#참조",
    "title": "depth-wise convolution",
    "section": "참조",
    "text": "참조\n\n\nhttps://nn.labml.ai/conv_mixer/index.html"
  },
  {
    "objectID": "dictionary/terms/strategy_profile.html",
    "href": "dictionary/terms/strategy_profile.html",
    "title": "strategy profile",
    "section": "",
    "text": "게임 이론에서 사용되는 용어\n게임에 참여하는 모든 플레이어에 대해서 그들이 각 상태에서 어떤 행동을 선택하는지에 대한 전략의 집합"
  },
  {
    "objectID": "dictionary/terms/strategy_profile.html#참조",
    "href": "dictionary/terms/strategy_profile.html#참조",
    "title": "strategy profile",
    "section": "참조",
    "text": "참조\n\nhttps://ko.d2l.ai/chapter_deep-learning-basics/weight-decay.html\nhttps://nn.labml.ai/cfr/index.html전"
  },
  {
    "objectID": "dictionary/terms/clustering.html",
    "href": "dictionary/terms/clustering.html",
    "title": "clustering",
    "section": "",
    "text": "딥러닝에서 군집화는 데이터를 비슷한 특성 또는 패턴을 가진 그룹으로 나누는 기술 또는 과정을 가리킵니다.\n클러스터링은 비지도 학습(Unsupervised Learning)의 일부로, 데이터에 숨겨진 구조를 발견하고 이를 사용하여 데이터를 그룹화합니다."
  },
  {
    "objectID": "dictionary/terms/clustering.html#참조",
    "href": "dictionary/terms/clustering.html#참조",
    "title": "clustering",
    "section": "참조",
    "text": "참조\n\n[딥러닝 clustering의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/seperator.html",
    "href": "dictionary/terms/seperator.html",
    "title": "seperator",
    "section": "",
    "text": "딥러닝에서 분할 토큰은 주로 언어 모델(예: GPT, BERT)을 사용하는 자연어 처리(NLP) 작업에서 텍스트를 토큰(token)으로 분할하고, 분할된 토큰 중 하나가 텍스트의 끝이나 다른 문맥적 경계를 나타내는 데 사용되는 특수한 토큰을 가리킵니다.\n일반적으로 언어 모델은 입력 텍스트를 토큰 단위로 분할하고, 각 토큰에 대한 임베딩 벡터를 생성하여 텍스트를 처리합니다. 이때 텍스트의 문맥을 이해하기 위해 각 문장이나 문단의 끝을 나타내는 특별한 토큰이 필요할 수 있습니다. 이 특별한 토큰은 문장 또는 문단 간의 경계를 표시하고 모델에게 언어의 문맥을 알리는 데 사용됩니다.\n예를 들어, 문장 “나는 고양이를 좋아해.”와 “고양이는 너무 귀여워.”가 있다고 가정해 봅시다. 이 두 문장을 토큰화하면 다음과 같을 수 있습니다:\n문장 1: [“나”, “는”, “고양이”, “를”, “좋아해”, “.”] 문장 2: [“고양이”, “는”, “너무”, “귀여워”, “.”] 여기서 마침표(“.”)는 각 문장의 끝을 나타내는 특수한 토큰으로 분할 토큰입니다. 이를 통해 모델은 두 문장이 서로 다른 문장임을 이해하고, 문장 간의 관계를 파악할 수 있습니다.\n또한, 분할 토큰은 대화형 언어 처리 작업에서 각 대화 발화를 구분하는 데도 사용될 수 있습니다. 대화 형식에서는 대개 대화 참가자의 발화 간에 경계를 나타내기 위해 분할 토큰을 사용합니다."
  },
  {
    "objectID": "dictionary/terms/seperator.html#참조",
    "href": "dictionary/terms/seperator.html#참조",
    "title": "seperator",
    "section": "참조",
    "text": "참조\n\n[딥러닝 분할토큰의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/inpainting.html",
    "href": "dictionary/terms/inpainting.html",
    "title": "inpainting",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/multi-image_batches.html",
    "href": "dictionary/terms/multi-image_batches.html",
    "title": "multi-image batches",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/argumentation.html",
    "href": "dictionary/terms/argumentation.html",
    "title": "argumentation",
    "section": "",
    "text": "딥러닝 영상처리 모델의 데이터 증강(augmentation)은 훈련 데이터를 다양하게 변형시켜 모델의 성능을 향상시키는 기법을 말합니다. 데이터 증강은 원본 훈련 데이터를 변형하여 새로운 데이터를 생성하고 이를 훈련에 활용하는 것입니다. 이로 인해 모델은 더 다양한 상황과 조건에서 일반화하고, 과적합을 줄이는데 도움이 됩니다.\n딥러닝 모델은 많은 훈련 데이터를 필요로 하기 때문에, 특히 영상처리 분야에서는 많은 양의 라벨링된 데이터를 구하기 어려울 수 있습니다. 데이터 증강은 이러한 한계를 극복하고, 작은 데이터셋으로도 더 효과적인 훈련이 가능하도록 돕습니다.\n영상처리 분야에서 흔히 사용되는 데이터 증강 방법들은 다음과 같습니다:\n데이터 증강은 더 다양한 데이터를 확보하고, 모델의 일반화 능력을 향상시켜 영상처리 애플리케이션에서 더 좋은 성능을 달성하는데 도움이 됩니다."
  },
  {
    "objectID": "dictionary/terms/argumentation.html#참조",
    "href": "dictionary/terms/argumentation.html#참조",
    "title": "argumentation",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 8월 4일). OpenAI. [딥러닝 영상처리 모델, augumentation 의미]. https://chat.openai.com/\nAPA 출처 생성기 (2023, 8월 4일). ESSAYREVIEW. https://essayreview.co.kr/citation-generator"
  },
  {
    "objectID": "dictionary/terms/reproducibility.html",
    "href": "dictionary/terms/reproducibility.html",
    "title": "reproducibility",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/configuration.html",
    "href": "dictionary/terms/configuration.html",
    "title": "configuration",
    "section": "",
    "text": "labml.ai 에서 파이토치 논문 구현 실습을 위해 미리 구현해둔 설정값들을 담은 Configs 모듈을 클래스로 사용 할 수 있다."
  },
  {
    "objectID": "dictionary/terms/configuration.html#참조",
    "href": "dictionary/terms/configuration.html#참조",
    "title": "configuration",
    "section": "참조",
    "text": "참조\n1.https://docs.labml.ai/api/configs.html"
  },
  {
    "objectID": "dictionary/terms/automatic_speech_recognition.html",
    "href": "dictionary/terms/automatic_speech_recognition.html",
    "title": "automatic speech recognition",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/chunk.html",
    "href": "dictionary/terms/chunk.html",
    "title": "chunk",
    "section": "",
    "text": "딥러닝에서 chunk는 일반적으로 데이터를 처리하고 분석하기 위해 입력 데이터를 덩어리로 나누는 작업을 가리키는 용어입니다. 이것은 주로 데이터의 효율적인 처리와 모델의 훈련에 관련이 있습니다\n예를 들어, 이미지 데이터를 처리할 때 이미지를 작은 패치로 나누어 각 패치에 대한 예측을 수행하고, 그 결과를 모아 최종 예측을 만들 수 있습니다. 이렇게 하면 메모리 효율성과 병렬 처리의 이점을 얻을 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/chunk.html#참조",
    "href": "dictionary/terms/chunk.html#참조",
    "title": "chunk",
    "section": "참조",
    "text": "참조\n\n[딥러닝 chunk의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/data_collator.html",
    "href": "dictionary/terms/data_collator.html",
    "title": "data collator",
    "section": "",
    "text": "데이터 콜레이터는 딥러닝에서 데이터를 준비하고 처리하는 역할을 하는 소프트웨어 또는 코드의 일부분을 가리키는 용어입니다.\n데이터 콜레이터는 특히 자연어 처리(NLP) 분야에서 많이 사용되며, 모델의 훈련 데이터를 처리하고 [배치]를 생성하는 작업을 수행합니다."
  },
  {
    "objectID": "dictionary/terms/data_collator.html#참조",
    "href": "dictionary/terms/data_collator.html#참조",
    "title": "data collator",
    "section": "참조",
    "text": "참조\n\n[data collator 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/vocoder.html",
    "href": "dictionary/terms/vocoder.html",
    "title": "vocoder",
    "section": "",
    "text": "오디오를 생성하는 모델은 로그 멜 스펙트로그램을 출력으로 생성하는 것이 일반적입니다. 따라서 사람이 인지할 수 있는 파형으로 변경하는 신경망인 보코더를 최종 출력단으로 사용합니다. 하지만 Bark와 같은 오디오 딥러닝 모델은 원시 음성 파형을 직접 생성하는 모델들은 별도의 보코더가 필요하지 않습니다."
  },
  {
    "objectID": "dictionary/terms/vocoder.html#참조",
    "href": "dictionary/terms/vocoder.html#참조",
    "title": "vocoder",
    "section": "참조",
    "text": "참조\n\nhttps://huggingface.co/learn/audio-course/en/chapter6/pre-trained_models"
  },
  {
    "objectID": "dictionary/terms/casual_language_model.html",
    "href": "dictionary/terms/casual_language_model.html",
    "title": "causal language model",
    "section": "",
    "text": "인과적 언어 모델링은 언어 모델링과 자연어 처리 분야에서 사용되는 모델 중 하나로, 주로 시퀀스 데이터에서 다음 단어나 토큰을 예측하기 위해 사용됩니다. 이 모델은 인과 관계(causal relationship)를 고려하여 시퀀스 데이터를 생성하거나 분석하는 데 중요한 역할을 합니다.\n이러한 이유로 인과적 언어 모델링은 다양한 응용 분야에서 데이터 분석, 예측, 생성, 이해, 추천 등의 작업을 효과적으로 수행하는 데 활용되며, 자연어 처리 분야에서 매우 중요한 역할을 합니다."
  },
  {
    "objectID": "dictionary/terms/casual_language_model.html#참조",
    "href": "dictionary/terms/casual_language_model.html#참조",
    "title": "causal language model",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [causal language model의 의미, causal language model을 사용하는 이유]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/weight_decay.html",
    "href": "dictionary/terms/weight_decay.html",
    "title": "weight decay",
    "section": "",
    "text": "과적합 문제를 해결하기 위한 방법 중 하나로서 학습된 모델의 복잡도를 줄이기 위해서 학습 중 가중치가 너무 큰 값을 가지지 않도록 손실 함수(loss function)에 가중치가 커지는 것에 대한 패널티를 부여합니다\n옵티마이저(optimizer)가 모든 파라미터를 다루기 때문에 옵티마이져에서 정규화 상수 하이퍼파라미터 지정으로 가중치를 감쇠시켜 구현합니다."
  },
  {
    "objectID": "dictionary/terms/weight_decay.html#참조",
    "href": "dictionary/terms/weight_decay.html#참조",
    "title": "weight decay",
    "section": "참조",
    "text": "참조\n\nhttps://ko.d2l.ai/chapter_deep-learning-basics/weight-decay.html\nhttps://nn.labml.ai/optimizers/index.html\nhttps://light-tree.tistory.com/216"
  },
  {
    "objectID": "dictionary/terms/sequence.html",
    "href": "dictionary/terms/sequence.html",
    "title": "sequence",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/attention_mask.html",
    "href": "dictionary/terms/attention_mask.html",
    "title": "attention mask",
    "section": "",
    "text": "어텐션 마스크는 주로 자연어 처리와 관련된 딥 러닝 모델에서 사용되는 개념입니다. 이것은 주로 트랜스포머(Transformer) 아키텍처와 관련이 있으며, 특히 어텐션 메커니즘(attention mechanism)을 사용하는 모델에서 주로 나타납니다.\n어텐션 메커니즘은 입력 시퀀스의 각 요소에 가중치를 할당하여 중요한 부분을 강조하고, 모델이 해당 부분에 집중하도록 돕는 기술입니다. 이 때, 어텐션 가중치(attention weights)를 결정하는 데 사용되는 정보가 어텐션 마스크입니다."
  },
  {
    "objectID": "dictionary/terms/attention_mask.html#참조",
    "href": "dictionary/terms/attention_mask.html#참조",
    "title": "attention mask",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [attention mask의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/causal_inference.html",
    "href": "dictionary/terms/causal_inference.html",
    "title": "causal inference",
    "section": "",
    "text": "인과추론에 대한 설명"
  },
  {
    "objectID": "dictionary/terms/causal_inference.html#참조",
    "href": "dictionary/terms/causal_inference.html#참조",
    "title": "causal inference",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/dataset.html",
    "href": "dictionary/terms/dataset.html",
    "title": "dataset",
    "section": "",
    "text": "데이터 세트는 딥러닝 모델을 훈련하고 평가하기 위해 사용되는 데이터의 집합을 가리킵니다.\n딥러닝 모델은 데이터셋을 사용하여 가중치를 조정하며 학습합니다. 모델은 입력 데이터와 해당 데이터에 대한 정답(레이블)사이의 관계를 학습하여 예측을 수행할 수 있도록 됩니다.\n딥러닝 데이터세트는 다양한 도메인에서 사용되며, 이미지 분류, 자연어 처리, 음성 인식, 추천 시스템, 자율 주행 자동차, 의료 이미지 분석 등이 있습니다."
  },
  {
    "objectID": "dictionary/terms/dataset.html#참조",
    "href": "dictionary/terms/dataset.html#참조",
    "title": "dataset",
    "section": "참조",
    "text": "참조\n\n[딥러닝 dataset의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/optimizer.html",
    "href": "dictionary/terms/optimizer.html",
    "title": "optimizer",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/loss_landscape.html",
    "href": "dictionary/terms/loss_landscape.html",
    "title": "loss landscape",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/loss_landscape.html#참조",
    "href": "dictionary/terms/loss_landscape.html#참조",
    "title": "loss landscape",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/discriminator.html",
    "href": "dictionary/terms/discriminator.html",
    "title": "discriminator",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/discriminator.html#참조",
    "href": "dictionary/terms/discriminator.html#참조",
    "title": "discriminator",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/logging.html",
    "href": "dictionary/terms/logging.html",
    "title": "logging",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/logging.html#참조",
    "href": "dictionary/terms/logging.html#참조",
    "title": "logging",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/step.html",
    "href": "dictionary/terms/step.html",
    "title": "step",
    "section": "",
    "text": "딥러닝에서 “step(스텝)”은 주로 훈련 알고리즘을 실행하거나 모델 파라미터를 업데이트하는 한 단위 작업을 나타냅니다. 데이터의 여러 배치(batch)를 사용하여 학습을 진행하며, 전체 학습 데이터를 여러 번(epoch) 사용하는 것이 일반적입니다. 반복 횟수는 학습의 효과와 모델의 수렴에 영향을 미칩니다."
  },
  {
    "objectID": "dictionary/terms/architecture.html",
    "href": "dictionary/terms/architecture.html",
    "title": "architecture",
    "section": "",
    "text": "아키텍처(architecture)는 다양한 분야에서 사용되는 용어로, 그 의미는 맥락에 따라 다를 수 있습니다. 주로 컴퓨터 과학 및 정보 기술 분야에서 아키텍처란 다음과 같은 의미로 사용됩니다.\n소프트웨어 아키텍처(Software Architecture): 소프트웨어 시스템의 구조와 구성 요소 간의 관계를 설계하는 것을 다룹니다. 소프트웨어 아키텍처는 소프트웨어 시스템의 모듈화, 계층화, 컴포넌트 간의 상호 작용, 데이터 흐름 등을 고려하여 시스템의 확장성, 유지보수성, 재사용성을 향상시키기 위해 사용됩니다."
  },
  {
    "objectID": "dictionary/terms/architecture.html#참조",
    "href": "dictionary/terms/architecture.html#참조",
    "title": "architecture",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [아키텍처의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/prompt.html",
    "href": "dictionary/terms/prompt.html",
    "title": "prompt",
    "section": "",
    "text": "딥러닝에서 prompt는 모델에게 어떤 작업을 수행하거나 정보를 요청하기 위한 입력 또는 지시사항을 가리킵니다. 이것은 모델이 어떤 작업을 수행해야 하는 것을 지정하는 텍스트 또는 문장으로 제공됩니다."
  },
  {
    "objectID": "dictionary/terms/prompt.html#참조",
    "href": "dictionary/terms/prompt.html#참조",
    "title": "prompt",
    "section": "참조",
    "text": "참조\n\n[딥러닝 prompt의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/batch.html",
    "href": "dictionary/terms/batch.html",
    "title": "batch",
    "section": "",
    "text": "배치는 데이터 처리 및 머신 러닝에서 중요한 개념 중 하나이며, 일괄 처리를 의미합니다. 주로 다음과 같은 두 가지 의미로 사용됩니다."
  },
  {
    "objectID": "dictionary/terms/batch.html#데이터-배치batch-of-data",
    "href": "dictionary/terms/batch.html#데이터-배치batch-of-data",
    "title": "batch",
    "section": "데이터 배치(Batch of Data):",
    "text": "데이터 배치(Batch of Data):\n데이터 배치란 데이터 집합(예: 이미지, 텍스트, 숫자 등)을 작은 덩어리로 나눈 것을 의미합니다. 각 덩어리를 배치라고 하며, 한 배치에는 여러 데이터 포인트가 포함됩니다.\n예를 들어, 이미지 처리에서 한 배치는 여러 장의 이미지로 구성됩니다. 배치를 사용하는 이유는 모델 학습의 효율성을 높이고, GPU 또는 CPU와 같은 하드웨어 가속기를 효율적으로 활용하기 위함입니다.\n배치 처리를 통해 여러 데이터 포인트를 동시에 처리하면 병렬 처리가 가능하며 학습 속도를 향상시킬 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/batch.html#일괄-처리-batch-processing",
    "href": "dictionary/terms/batch.html#일괄-처리-batch-processing",
    "title": "batch",
    "section": "일괄 처리( Batch Processing):",
    "text": "일괄 처리( Batch Processing):\n일괄 처리는 컴퓨터 과학 및 데이터 처리 분야에서 사용되는 용어로, 데이터를 일정 크기의 묶음으로 처리하는 방식을 의미합니다.\n대규모 데이터 처리 작업을 보다 효율적으로 수행하고, 데이터베이스에서 쿼리를 실행하거나 배치 작업을 수행하는 등의 다양한 컴퓨팅 작업에 사용됩니다.\n예를 들어, 머신 러닝에서 데이터를 학습할 때, 훈련 데이터 세트를 작은 배치로 나누어 각 배치를 모델에 공급하여 학습합니다. 이렇게 하면 모델이 전체 데이터 세트를 한 번에 처리하지 않고, 배치 단위로 처리하므로 학습 과정이 효율적으로 이루어집니다.\n데이터를 배치로 나누는 방식은 머신 러닝 모델 및 작업에 따라 다를 수 있으며, 배치 크기(batch size)는 모델의 성능과 학습 속도에 영향을 미칠 수 있습니다. 적절한 배치 크기를 선택하는 것은 모델 학습 및 성능 조정에서 중요한 요소 중 하나입니다."
  },
  {
    "objectID": "dictionary/terms/batch.html#참조",
    "href": "dictionary/terms/batch.html#참조",
    "title": "batch",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [batch의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/feature_extraction.html",
    "href": "dictionary/terms/feature_extraction.html",
    "title": "feature extraction",
    "section": "",
    "text": "딥러닝에서 특성 추출은 데이터로부터 유용한 정보를 추출하고 이를 기계 학습 모델이나 딥러닝 모델에 입력으로 제공하기 위한 과정을 가리킵니다.\n이 과정은 주로 원시 데이터로부터 고수준 특징이나 표현을 추출하여 데이터를 더 쉽게 처리하고 모델의 성능을 향상시키는 데 사용됩니다"
  },
  {
    "objectID": "dictionary/terms/feature_extraction.html#참조",
    "href": "dictionary/terms/feature_extraction.html#참조",
    "title": "feature extraction",
    "section": "참조",
    "text": "참조\n\n[feature extraction의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/fine_tunning_1.html",
    "href": "dictionary/terms/fine_tunning_1.html",
    "title": "fine tunning",
    "section": "",
    "text": "딥러닝에서 파인튜닝은 미리 훈련된(pre-trained) 모델을 가져와서 추가적인 훈련을 수행하는 과정을 가리킵니다.\n이것은 주로 전이 학습(transfer learning)의 한 형태로 사용되며, 이미 훈련된 모델의 일부 레이어나 파라미터를 조정하여 새로운 작업에 맞게 모델을 개선하는데 사용됩니다."
  },
  {
    "objectID": "dictionary/terms/fine_tunning_1.html#참조",
    "href": "dictionary/terms/fine_tunning_1.html#참조",
    "title": "fine tunning",
    "section": "참조",
    "text": "참조\n\n[fine tunning의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/entity.html",
    "href": "dictionary/terms/entity.html",
    "title": "entity",
    "section": "",
    "text": "소프트웨어 엔티티는 소프트웨어 시스템 또는 소프트웨어 구성 요소 내에서 독립적으로 식별 가능하며, 개별적인 역할 또는 기능을 수행하는 추상적인 개체를 나타냅니다.\n이 용어는 소프트웨어 엔지니어링 및 소프트웨어 설계 분야에서 주로 사용되며, 소프트웨어를 구조화하고 이해하는 데 도움이 됩니다. 대표적인 소프트웨어 엔티티는 클래스(Class), 객체(Object), 모듈(Module) 등이 있습니다.\n소프트웨어 엔티티는 소프트웨어 시스템을 설계, 개발 및 관리하는 데 중요한 역할을 합니다. 이러한 엔티티는 소프트웨어의 모듈화, 재사용성, 유지 보수 등과 관련된 다양한 소프트웨어 엔지니어링 원칙을 따르는 데 도움이 됩니다."
  },
  {
    "objectID": "dictionary/terms/entity.html#참조",
    "href": "dictionary/terms/entity.html#참조",
    "title": "entity",
    "section": "참조",
    "text": "참조\n\n[소프트웨어 entity의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/crob.html",
    "href": "dictionary/terms/crob.html",
    "title": "crob",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/spectrogram.html",
    "href": "dictionary/terms/spectrogram.html",
    "title": "spectrogram",
    "section": "",
    "text": "스펙트로그램(Spectrogram)은 시간과 주파수를 2차원 그래픽 형태로 나타내는 시각화 도구입니다. 스펙트로그램은 주로 오디오 신호, 음악, 음성 등의 시계열 데이터를 분석하고 표현하는 데 사용됩니다.\n스펙트로그램은 시간에 따른 주파수 변화를 시각적으로 보여줍니다. 가로 축은 시간을, 세로 축은 주파수를 나타내며, 플롯의 색상 농도는 해당 시간과 주파수에서의 신호 강도를 나타냅니다."
  },
  {
    "objectID": "dictionary/terms/spectrogram.html#참조",
    "href": "dictionary/terms/spectrogram.html#참조",
    "title": "spectrogram",
    "section": "참조",
    "text": "참조\n\n[spectogram의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/denoise.html",
    "href": "dictionary/terms/denoise.html",
    "title": "denoise",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "가이드 문서",
    "section": "",
    "text": "이 문서는 용어 사전을 사용하는 방법과 용어를 등록하기 위한 정보를 전달하기 위해 작성되었습니다.\n\n\n용어 사전에 등록된 용어 설명 페이지의 구조를 알아봅니다. 용어 사전에 등록된 상세 페이지는 영어와 한국어 용어 이름으로 구성됩니다. 아래의 archirecture로 등록된 상세 페이지를 통해 좀 더 자세히 알아보겠습니다. architecture라는 영어에 대한 한국어 용어명이 바로 아키텍처로 표시됩니다. 바로 밑에는 마크다운 배지로 영어 용어가 사용되었던 원본 문서 링크가 연결되었습니다.\n\narcitrecture용어에 대한 웹페이지는 용어 사전 소스파일의 archirecture.qmd파일의 정보로 생성됩니다. 코드의 상단의 title과 description은 각각 용어의 영어와 한국어 표현을 나타내고 url은 용어가 나온 원본 문서의 위치를 의미합니다. 용어 사전의 정보가 코드로 어떻게 표현되었는지 아래의 코드에서 확인할 수 있습니다.\n---\ntitle: 'architecture'\ndescription: '아키텍처'\nurl: https://huggingface.co/docs/transformers/index\n---\n&lt;a href=\"https://huggingface.co/docs/transformers/index\" target=\"_blank\"&gt;\n    &lt;img loading=\"lazy\"\n     alt=\"src: HuggingFace Transformer\"\n     src=\"https://img.shields.io/badge/문서-Huggingface_Transformer-blue\" &gt;\n&lt;/a&gt;\n딥러닝 분야에 따라 동일한 용어가 다른 뜻으로 사용되거나 문서에 따라 한국어 번역 방식이 다를 수 있습니다. 각 문서에는 원본 문서에 대한 링크가 있기 때문에 원본 문서별로 일관성 있게 번역 용어가 사용될 수 있습니다. 마크다운 배지는 image shields io를 이용합니다. 배지의 생성 방식은 코드에서 이미지 소스의 주소를 잘 살펴보면 이해할 수 있습니다. 마크다운 배지에 표시되는 문구는 img shields io에 전달하는 URL 정보와 동일하네요. 위의 코드에서는 문서-Huggingface_Transformer-blue 를 이미지 위치로 전달했고 문서, Huggingface_Transformer, blue의 3개의 배지 정보는 구분자 -로 분리됩니다. 공백을 _로 표시했음을 주의하세요."
  },
  {
    "objectID": "guide.html#용어-페이지-구조",
    "href": "guide.html#용어-페이지-구조",
    "title": "가이드 문서",
    "section": "",
    "text": "용어 사전에 등록된 용어 설명 페이지의 구조를 알아봅니다. 용어 사전에 등록된 상세 페이지는 영어와 한국어 용어 이름으로 구성됩니다. 아래의 archirecture로 등록된 상세 페이지를 통해 좀 더 자세히 알아보겠습니다. architecture라는 영어에 대한 한국어 용어명이 바로 아키텍처로 표시됩니다. 바로 밑에는 마크다운 배지로 영어 용어가 사용되었던 원본 문서 링크가 연결되었습니다.\n\narcitrecture용어에 대한 웹페이지는 용어 사전 소스파일의 archirecture.qmd파일의 정보로 생성됩니다. 코드의 상단의 title과 description은 각각 용어의 영어와 한국어 표현을 나타내고 url은 용어가 나온 원본 문서의 위치를 의미합니다. 용어 사전의 정보가 코드로 어떻게 표현되었는지 아래의 코드에서 확인할 수 있습니다.\n---\ntitle: 'architecture'\ndescription: '아키텍처'\nurl: https://huggingface.co/docs/transformers/index\n---\n&lt;a href=\"https://huggingface.co/docs/transformers/index\" target=\"_blank\"&gt;\n    &lt;img loading=\"lazy\"\n     alt=\"src: HuggingFace Transformer\"\n     src=\"https://img.shields.io/badge/문서-Huggingface_Transformer-blue\" &gt;\n&lt;/a&gt;\n딥러닝 분야에 따라 동일한 용어가 다른 뜻으로 사용되거나 문서에 따라 한국어 번역 방식이 다를 수 있습니다. 각 문서에는 원본 문서에 대한 링크가 있기 때문에 원본 문서별로 일관성 있게 번역 용어가 사용될 수 있습니다. 마크다운 배지는 image shields io를 이용합니다. 배지의 생성 방식은 코드에서 이미지 소스의 주소를 잘 살펴보면 이해할 수 있습니다. 마크다운 배지에 표시되는 문구는 img shields io에 전달하는 URL 정보와 동일하네요. 위의 코드에서는 문서-Huggingface_Transformer-blue 를 이미지 위치로 전달했고 문서, Huggingface_Transformer, blue의 3개의 배지 정보는 구분자 -로 분리됩니다. 공백을 _로 표시했음을 주의하세요."
  },
  {
    "objectID": "guide.html#준비-하기",
    "href": "guide.html#준비-하기",
    "title": "가이드 문서",
    "section": "준비 하기",
    "text": "준비 하기\n딥러닝 용어 사전은 quarto를 이용하여 작성되었습니다. Quarto 웹사이트로 이동하여 프로그램을 설치해주세요. 설치가 완료되었다면 이제 본격적으로 문서를 작성할 준비를 끝났습니다.\n용어 사전의 구조를 살펴볼까요? 용어 사전에 작성된 용어들은 아래의 terms폴더에 하나의 파일 형태로 생성됩니다. 예를 들어 convolution을 수정하는 경우 terms 폴더에 생성된 convolution.qmd파일을 수정합니다.\n├── glossary\n│   ├── dictionary\n|   │   └── terms\n|   |       ├── architecture.qmd\n|   |               ...\n|   |       ├── convolution.qmd\n|   |               ...\n|   │       └── speech_enhancement.qmd\n\n새로운 용어 작성\n앗, 제가 찾는 용어가 없습니다! 새로운 용어를 등록하는 경우 terms폴더에 등록하려는 용어명과 동일한 파일명으로 qmd파일을 생성합니다.\n\n\n\n\n\n\n신규용어 파일명 생성 규칙\n\n\n\n\n신규 용어는 모두 소문자이며 공백은 _로 변경하여 파일명을 생성합니다.\n파일명은 모두 소문자로 작성하고 공백은 _로 대체하여 사용하고 있습니다.\n같은 파일명이 있는 경우 _1, _2와 같이 postfix를 붙여 추가합니다.\n\ndirectory.qmd\ndirectory_1.qmd\n\n\n\n\n신규 용어 파일명 생성 규칙에 맞게 파일을 생성한 후 문서 작성을 시작합니다."
  },
  {
    "objectID": "guide.html#문서-작성",
    "href": "guide.html#문서-작성",
    "title": "가이드 문서",
    "section": "문서 작성",
    "text": "문서 작성\n딥러닝 용어사전은 깃헙 리포지토리(repository)로 관리되기 때문에 문서 작성을 위해서 Pseudo-Lab 깃 리포지토리를 Fork하여 문서를 작업합니다. Fork된 리포지토리에서 수정할 웹페이지의 html 파일명과 동일한 이름의 qmd 확장자 파일을 열어 문서를 수정할 수 있습니다.\n문서 작성은 기본적으로 markdown언어를 이용하여 작성하지만 markdown에 익숙하지 않은 경우라면 Visual Code를 이용하는 것을 추천드립니다. Visual Code의 extention 메뉴에서 quarto를 검색하여 익스텐션을 설치할 수 있으며 Figure 1 의 quarto 익스텐션은 GUI환경에서 문서를 쉽게 작성있도록 도와줍니다.\n\n\n\nFigure 1: Quarto Extention\n\n\nVisual Code에서 자신이 수정할 문서를 선택하고 마우스 우측 버튼을 누르면 Edit in Visual Mode를 선택할 수 있습니다. 선택을 완료하면 markdown형태로 표시되던 문서가 wysiwyg1 에디터로 변경되고 Figure 2 와 같이 메뉴를 통해서 문서를 작성할 수 있습니다.\n\n\n\nFigure 2: VS Code Visual Mode"
  },
  {
    "objectID": "guide.html#미리-보기",
    "href": "guide.html#미리-보기",
    "title": "가이드 문서",
    "section": "미리 보기",
    "text": "미리 보기\nVS Code등의 에디터를 사용했다면 자신이 수정한 내용을 바로 확인할 수 있습니다. 만일 GUI방식으로 문서를 작성하지 않는 경우 아래의 quarto preview명령을 이용해서 문서를 로컬 웹브라우져에서 확인할 수 있습니다.\ncd glossary\nquarto preview\npreview명령을 수행하면 로컬 컴퓨터에 브라우져가 실행되고 수정된 웹사이트를 바로 확인할 수 있습니다. 변경사항에 문제가 없다면 이제 PR(Pull Request)를 진행할 수 있습니다."
  },
  {
    "objectID": "guide.html#footnotes",
    "href": "guide.html#footnotes",
    "title": "가이드 문서",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwysiwyg(What You See is What You Get)의 약자로 문서 작성 방법을 GUI로 구현한 에디터 입니다.↩︎\nhttps://won.hashnode.dev/a-comprehensive-guide-to-mastering-github-pr-reviews↩︎\nhttp://www.publickrdata.com/blog/74/↩︎"
  },
  {
    "objectID": "contribution.html",
    "href": "contribution.html",
    "title": "기여하신 분",
    "section": "",
    "text": "용어 사전에 기여해주신 모든 분들께 감사드립니다. 새로운 기여자와 팀을 찾습니다. 함께 해주실꺼죠?\n\n\n허깅페이스와 다른 딥러닝 관련 프로젝트를 한글화하고 있습니다. 관련 프로젝트를 번역하는 과정에서 용어를 일관성 있게 번역하기 위해서 사전을 만들고 있습니다.\n\n\n\n이름\n조직\n소개\n\n\n\n\n서원형\nPseudo Lab. Huggingface KREW\n\n\n\n손기훈\nPseudo Lab. Huggingface KREW\n\n\n\n심소현\nPseudo Lab. Huggingface KREW\n\n\n\n양성모\nPseudo Lab. Huggingface KREW\nGithub\n\n\n윤현서\nPseudo Lab. Huggingface KREW\n\n\n\n정우준\nPseudo Lab. Huggingface KREW\nGithub\n\n\n한나연\nPseudo Lab. Huggingface KREW\n\n\n\n\n\n\n\n\n\n\n\n이름\n조직\n소개\n\n\n\n\n이홍규\nPseudo Lab. Diffuser 프로젝트\n\n\n\n박성수\nPseudo Lab. Diffuser 프로젝트\n\n\n\n이준형\nPseudo Lab. Diffuser 프로젝트\n\n\n\n윤형석\nPseudo Lab. Diffuser 프로젝트\n\n\n\n김지환\nPseudo Lab. Diffuser 프로젝트\n\n\n\n안혜민\nPseudo Lab. Diffuser 프로젝트\n\n\n\n\n\n\n\n가짜연구소 인과추론팀은 한국어 자료가 많지 않은 인과추론을 많은 분들이 쉽게 접하실 수 있도록 기여하고자 합니다!\n\n\n\n이름\n조직\n소개\n\n\n\n\n신진수\nPseudo Lab. Causal Inference Team\nGithub / Linkedin"
  },
  {
    "objectID": "contribution.html#huggingface-krew",
    "href": "contribution.html#huggingface-krew",
    "title": "기여하신 분",
    "section": "",
    "text": "허깅페이스와 다른 딥러닝 관련 프로젝트를 한글화하고 있습니다. 관련 프로젝트를 번역하는 과정에서 용어를 일관성 있게 번역하기 위해서 사전을 만들고 있습니다.\n\n\n\n이름\n조직\n소개\n\n\n\n\n서원형\nPseudo Lab. Huggingface KREW\n\n\n\n손기훈\nPseudo Lab. Huggingface KREW\n\n\n\n심소현\nPseudo Lab. Huggingface KREW\n\n\n\n양성모\nPseudo Lab. Huggingface KREW\nGithub\n\n\n윤현서\nPseudo Lab. Huggingface KREW\n\n\n\n정우준\nPseudo Lab. Huggingface KREW\nGithub\n\n\n한나연\nPseudo Lab. Huggingface KREW"
  },
  {
    "objectID": "contribution.html#허깅페이스-diffuser-번역-프로젝트",
    "href": "contribution.html#허깅페이스-diffuser-번역-프로젝트",
    "title": "기여하신 분",
    "section": "",
    "text": "이름\n조직\n소개\n\n\n\n\n이홍규\nPseudo Lab. Diffuser 프로젝트\n\n\n\n박성수\nPseudo Lab. Diffuser 프로젝트\n\n\n\n이준형\nPseudo Lab. Diffuser 프로젝트\n\n\n\n윤형석\nPseudo Lab. Diffuser 프로젝트\n\n\n\n김지환\nPseudo Lab. Diffuser 프로젝트\n\n\n\n안혜민\nPseudo Lab. Diffuser 프로젝트"
  },
  {
    "objectID": "contribution.html#인과추론팀",
    "href": "contribution.html#인과추론팀",
    "title": "기여하신 분",
    "section": "",
    "text": "가짜연구소 인과추론팀은 한국어 자료가 많지 않은 인과추론을 많은 분들이 쉽게 접하실 수 있도록 기여하고자 합니다!\n\n\n\n이름\n조직\n소개\n\n\n\n\n신진수\nPseudo Lab. Causal Inference Team\nGithub / Linkedin"
  },
  {
    "objectID": "dictionary/terms/instance.html",
    "href": "dictionary/terms/instance.html",
    "title": "instance",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/mixed_precision.html",
    "href": "dictionary/terms/mixed_precision.html",
    "title": "mixed precision",
    "section": "",
    "text": "혼합 정밀도는 딥러닝 모델의 연산을 고정 정밀도와 부동 소수점 정밀도를 혼합하여 처리하는 기술을 가리킵니다. 이 기술은 딥러닝 모델의 효율성을 높이고 계산 리소스를 절약하기 위해 사용됩니다."
  },
  {
    "objectID": "dictionary/terms/mixed_precision.html#참조",
    "href": "dictionary/terms/mixed_precision.html#참조",
    "title": "mixed precision",
    "section": "참조",
    "text": "참조\n\n[mixed precision의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/convolution.html",
    "href": "dictionary/terms/convolution.html",
    "title": "convolution",
    "section": "",
    "text": "합성곱은 주로 이미지 처리 및 패턴 인식 작업에서 사용되며, 입력 데이터와 필터(또는 커널) 간의 연산을 나타냅니다. 합성곱은 필터를 입력 데이터 위를 슬라이딩하면서 계산됩니다. 이렇게 필터를 이동시키면 입력 데이터의 다양한 위치에서 특징을 추출할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/convolution.html#참조",
    "href": "dictionary/terms/convolution.html#참조",
    "title": "convolution",
    "section": "참조",
    "text": "참조\n\n[합성곱의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/repository.html",
    "href": "dictionary/terms/repository.html",
    "title": "repository",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/autoregressive_model.html",
    "href": "dictionary/terms/autoregressive_model.html",
    "title": "autoregressive model",
    "section": "",
    "text": "자연어 처리와 시계열 데이터 분석과 같은 다양한 분야에서 사용되는 자기회귀 모델은 시계열 데이터나 순차적 데이터를 모델링하기 위한 통계적 또는 머신 러닝 모델의 일종입니다. 이 모델은 현재 시점의 데이터를 이전 시점의 데이터와 관련하여 예측하거나 모델링하는 데 사용됩니다.\n자기회귀 모델은 시계열 예측, 확률적 시계열 생성, 자연어 처리에서 언어 모델링 등 다양한 응용 분야에서 사용됩니다. 대표적으로 자기회귀 모델로 알려진 것 중 하나는 자연어 처리에서 사용되는 “언어 모델(Language Model)”입니다. 이 모델은 문장을 생성하거나 다음 단어를 예측하는 데 자주 사용됩니다.\n예를 들어, 자연어 처리에서 GPT-3와 같은 모델은 자기회귀 모델의 한 유형으로, 이전 단어들을 사용하여 다음 단어를 예측하는 방식으로 텍스트를 생성합니다."
  },
  {
    "objectID": "dictionary/terms/autoregressive_model.html#참조",
    "href": "dictionary/terms/autoregressive_model.html#참조",
    "title": "autoregressive model",
    "section": "참조",
    "text": "참조\n\nChatgpt3.5. (2023, 10월 8일). OpenAI. [autoregressive model의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/gradient.html",
    "href": "dictionary/terms/gradient.html",
    "title": "gradient",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/load.html",
    "href": "dictionary/terms/load.html",
    "title": "load",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/method.html",
    "href": "dictionary/terms/method.html",
    "title": "method",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/logit.html",
    "href": "dictionary/terms/logit.html",
    "title": "logit",
    "section": "",
    "text": "Logit(로짓) 함수는 0에서 1까지의 확률값과 -∞에서 ∞ 사이의 확률값을 표현해주는 함수입니다. 표준 로지스틱 분포 즉 Y축에서 0과 1 사이를 제한하는 시그모이드의 역함수라 로짓 함수는 X축에서 0과 1 사이의 자연로그 함수로 표현됩니다. 이러한 특성 때문에 log-odds 함수이며 확률 p가 0과 1사이에 있기에 로짓함수는 확률을 이해하는 데 가장 일반적으로 사용됩니다."
  },
  {
    "objectID": "dictionary/terms/logit.html#참조",
    "href": "dictionary/terms/logit.html#참조",
    "title": "logit",
    "section": "참조",
    "text": "참조\n\nhttps://deepai.org/machine-learning-glossary-and-terms/logit#:~:text=A%20Logit%20function%2C%20also%20known%20as%20the%20log-,1%20across%20the%20Y-axis%2C%20rather%20than%20the%20X-axis.\nhttps://en.wikipedia.org/wiki/Logit"
  },
  {
    "objectID": "dictionary/terms/method_1.html",
    "href": "dictionary/terms/method_1.html",
    "title": "model",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/silent_error.html",
    "href": "dictionary/terms/silent_error.html",
    "title": "silent error",
    "section": "",
    "text": "조용한 오류(silent error)는 프로그램 또는 코드 실행 중에 발생하는 오류 중에서 명시적인 오류 메시지나 경고 없이 발생하며, 프로그램이 비정상적으로 동작하거나 예상치 못한 결과를 내는 오류를 가리킵니다. 이러한 오류는 디버깅하기 어려우며, 프로그램의 안정성과 신뢰성에 영향을 미칠 수 있습니다.\n조용한 오류가 발생하는 경우, 프로그램은 오류가 있음에도 불구하고 실행을 계속하며, 이로 인해 예기치 않은 결과를 생성할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/silent_error.html#참조",
    "href": "dictionary/terms/silent_error.html#참조",
    "title": "silent error",
    "section": "참조",
    "text": "참조\n\n[silent error의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/push.html",
    "href": "dictionary/terms/push.html",
    "title": "push",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/hidden_state.html",
    "href": "dictionary/terms/hidden_state.html",
    "title": "hidden state",
    "section": "",
    "text": "인공 신경망에서 은닉 상태는 모델의 내부 상태를 나타내는 개념입니다. 이것은 네트워크의 중간 레이어 또는 히든 레이어에서 발생하는 활성화 값의 집합을 가리킵니다.\n은닉 상태는 딥러닝 모델에서 중요한 역할을 하며, 모델의 학습 및 예측 과정에서 정보를 전달하고 변환하는 데 사용됩니다. 구조와 특성은 모델의 종류와 목적에 따라 달라질 수 있으며, 모델을 이해하고 해석하는 데 중요한 개념 중 하나입니다."
  },
  {
    "objectID": "dictionary/terms/hidden_state.html#참조",
    "href": "dictionary/terms/hidden_state.html#참조",
    "title": "hidden state",
    "section": "참조",
    "text": "참조\n\n[hidden state의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/label_smoothing_function.html",
    "href": "dictionary/terms/label_smoothing_function.html",
    "title": "label smoothing function",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/label_smoothing_function.html#참조",
    "href": "dictionary/terms/label_smoothing_function.html#참조",
    "title": "label smoothing function",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/pooling.html",
    "href": "dictionary/terms/pooling.html",
    "title": "pooling",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/pooling.html#참조",
    "href": "dictionary/terms/pooling.html#참조",
    "title": "pooling",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/pading.html",
    "href": "dictionary/terms/pading.html",
    "title": "pading",
    "section": "",
    "text": "딥러닝에서 패딩은 주로 이미지 또는 시퀀스 데이터와 관련된 작업에서 사용되는 용어입니다. 패딩은 데이터의 크기나 모양을 조절하고 특정 형식 또는 요구사항을 충족시키기 위해 추가된 빈 공간 또는 값입니다."
  },
  {
    "objectID": "dictionary/terms/pading.html#참조",
    "href": "dictionary/terms/pading.html#참조",
    "title": "pading",
    "section": "참조",
    "text": "참조\n\n[딥러닝 padding의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/inference.html",
    "href": "dictionary/terms/inference.html",
    "title": "inference",
    "section": "",
    "text": "딥러닝에서 추론은 모델이 훈련된 후에 새로운 입력 데이터에 대한 예측, 분류, 또는 출력을 생성하는 과정을 가리킵니다. 이 과정은 모델이 학습한 지식을 활용하여 새로운 데이터에 대한 결정을 내리는 것을 의미하며, 주로 모델의 실제 운영 및 사용 단계에서 발생합니다."
  },
  {
    "objectID": "dictionary/terms/inference.html#참조",
    "href": "dictionary/terms/inference.html#참조",
    "title": "inference",
    "section": "참조",
    "text": "참조\n\n[딥러닝 inference의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/custom.html",
    "href": "dictionary/terms/custom.html",
    "title": "custom",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/branch.html",
    "href": "dictionary/terms/branch.html",
    "title": "branch",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/token.html",
    "href": "dictionary/terms/token.html",
    "title": "token",
    "section": "",
    "text": "딥러닝에서 “token(토큰)”은 텍스트 데이터를 작은 단위로 분할하는 과정에서 나오는 기본적인 단위를 나타냅니다. 이 단위는 일반적으로 단어, 부분 단어, 문자 또는 하위 단어 단위일 수 있습니다. 토큰화(tokenization)는 텍스트 데이터를 이러한 토큰으로 분해하는 과정을 말합니다."
  },
  {
    "objectID": "dictionary/terms/token.html#참조",
    "href": "dictionary/terms/token.html#참조",
    "title": "token",
    "section": "참조",
    "text": "참조\n\n[딥러닝 token의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/iteration.html",
    "href": "dictionary/terms/iteration.html",
    "title": "iteration",
    "section": "",
    "text": "1 epoch(에폭)을 마치는 데 필요한 미니 배치의 수입니다. 하나의 epoch에 대해서 몇번 반복해서 학습할 지 나타내는 횟수입니다.\n예를 들어 1500개의 데이터에 대해서 배치 사이즈 100으로 15개의 미니 배치로 나누었을 때 1 에폭을 완료 하기 위해서는 15-iteration 반복이 필요하며 15번의 파라미터 업데이트가 진행 됩니다"
  },
  {
    "objectID": "dictionary/terms/iteration.html#참조",
    "href": "dictionary/terms/iteration.html#참조",
    "title": "iteration",
    "section": "참조",
    "text": "참조\n\nhttps://losskatsu.github.io/machine-learning/epoch-batch/#3-epoch%EC%9D%98-%EC%9D%98%EB%AF%B8"
  },
  {
    "objectID": "dictionary/terms/speech_enhancement.html",
    "href": "dictionary/terms/speech_enhancement.html",
    "title": "speech enhancement",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/convolution_1.html",
    "href": "dictionary/terms/convolution_1.html",
    "title": "convolution",
    "section": "",
    "text": "컨볼루션은 주로 이미지 처리 및 패턴 인식 작업에서 사용되며, 입력 데이터와 필터(또는 커널) 간의 연산을 나타냅니다.\n컨볼루션은 필터를 입력 데이터 위를 슬라이딩하면서 계산됩니다. 이렇게 필터를 이동시키면 입력 데이터의 다양한 위치에서 특징을 추출할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/convolution_1.html#참조",
    "href": "dictionary/terms/convolution_1.html#참조",
    "title": "convolution",
    "section": "참조",
    "text": "참조\n\n[합성곱의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/learning.html",
    "href": "dictionary/terms/learning.html",
    "title": "learning",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/resume.html",
    "href": "dictionary/terms/resume.html",
    "title": "resume",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/overfitting.html",
    "href": "dictionary/terms/overfitting.html",
    "title": "over fitting",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/overfitting.html#참조",
    "href": "dictionary/terms/overfitting.html#참조",
    "title": "over fitting",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/directory.html",
    "href": "dictionary/terms/directory.html",
    "title": "directory",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/lesson.html",
    "href": "dictionary/terms/lesson.html",
    "title": "lesson",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/hyperparameter.html",
    "href": "dictionary/terms/hyperparameter.html",
    "title": "hyperparameter",
    "section": "",
    "text": "딥러닝에서 하이퍼파라미터는 모델의 학습 과정 및 구조를 제어하고 조정하는 매개 변수를 가리킵니다. 학습률(learning rate), 드롭아웃(dropout), 에폭수(epoch)는 모델 학습을 효율적으로 진행하기 위한 하이퍼파라미터로 자주 사용됩니다. 하이퍼파라미터의 설정은 모델의 성능과 학습 속도에 큰 영향을 미칩니다."
  },
  {
    "objectID": "dictionary/terms/hyperparameter.html#참조",
    "href": "dictionary/terms/hyperparameter.html#참조",
    "title": "hyperparameter",
    "section": "참조",
    "text": "참조\n\n[hyperparameter의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/jansen_shannon_divergence.html",
    "href": "dictionary/terms/jansen_shannon_divergence.html",
    "title": "Jansen-Shannon divergence",
    "section": "",
    "text": "확률 이론 및 통계학에서 젠슨-셰넌 다이버전스는 두 확률 분포 사이의 유사성을 측정하는 방법입니다.\n평균에 대한 정보 반경(Information Radius) 또는 평균에 대한 총 다이버전스로도 알려져 있습니다.\n대칭이고 항상 유한한 값을 갖는 것을 포함하여 몇 가지 주목할 만한 (그리고 유용한) 차이가 있는 Kullabck-Leibler 다이버전스를 기반으로 합니다.\n잰슨-섀넌 발산의 제곱근은 종종 잰슨-섀넌 거리라고 불리는 메트릭입니다."
  },
  {
    "objectID": "dictionary/terms/jansen_shannon_divergence.html#참조",
    "href": "dictionary/terms/jansen_shannon_divergence.html#참조",
    "title": "Jansen-Shannon divergence",
    "section": "참조",
    "text": "참조\n\nhttps://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\nhttps://nn.labml.ai/gan/wasserstein/index.html"
  },
  {
    "objectID": "dictionary/terms/arguments.html",
    "href": "dictionary/terms/arguments.html",
    "title": "arguments",
    "section": "",
    "text": "소프트웨어에서 인수는 프로그램 또는 스크립트를 실행할 때 커맨드 라인에서 전달하는 정보나 매개 변수를 나타냅니다. 이러한 인수는 프로그램이 실행되는 동안에 사용되거나 처리되며, 프로그램이 원하는 동작을 수행하도록 지시하는 데 사용됩니다.\n인수는 프로그램이 특정 동작 또는 설정을 변경하도록 하는 옵션을 제공할 수 있습니다. 예를 들어, 명령줄에서 –verbose 플래그를 사용하여 프로그램이 자세한 로그를 출력하도록 지시할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/arguments.html#참조",
    "href": "dictionary/terms/arguments.html#참조",
    "title": "arguments",
    "section": "참조",
    "text": "참조\n\n[소프트웨어 arguments의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/earth_mover_distance.html",
    "href": "dictionary/terms/earth_mover_distance.html",
    "title": "earth mover distance",
    "section": "",
    "text": "Earth mover는 흙을 파는 기계이다."
  },
  {
    "objectID": "dictionary/terms/earth_mover_distance.html#참조",
    "href": "dictionary/terms/earth_mover_distance.html#참조",
    "title": "earth mover distance",
    "section": "참조",
    "text": "참조\n\nhttps://en.wikipedia.org/wiki/Earth_mover%27s_distance\nhttps://nn.labml.ai/gan/wasserstein/index.html"
  },
  {
    "objectID": "dictionary/terms/l2_norm.html",
    "href": "dictionary/terms/l2_norm.html",
    "title": "L2 norm",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/l2_norm.html#참조",
    "href": "dictionary/terms/l2_norm.html#참조",
    "title": "L2 norm",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/generator.html",
    "href": "dictionary/terms/generator.html",
    "title": "generator",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/generator.html#참조",
    "href": "dictionary/terms/generator.html#참조",
    "title": "generator",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/xlm-roberta.html",
    "href": "dictionary/terms/xlm-roberta.html",
    "title": "XLM-RoBERTa",
    "section": "",
    "text": "XLM-RoBERTa 모델은 Unsupervised Cross-lingual Representation Learning at Scale에서 제안 되었습니다 . 2019년에 출시된 Facebook의 RoBERTa 모델을 기반으로 합니다. 2.5TB의 필터링된 CommonCrawl 데이터에서 훈련된 대규모 다국어 언어 모델입니다.\nRoBERTa 모델은 RoBERTa: A Robustly Optimized BERT Pretraining Approach에서 제안되었습니다. 2018년에 출시된 Google의 BERT 모델을 기반으로 합니다. BERT를 기반으로 하며 주요 하이퍼파라미터를 수정하여 다음 문장 사전 훈련 목표를 제거하고 훨씬 더 큰 미니 배치 및 학습 속도로 훈련합니다."
  },
  {
    "objectID": "dictionary/terms/xlm-roberta.html#section",
    "href": "dictionary/terms/xlm-roberta.html#section",
    "title": "XLM-RoBERTa",
    "section": "",
    "text": "XLM-RoBERTa 모델은 Unsupervised Cross-lingual Representation Learning at Scale에서 제안 되었습니다 . 2019년에 출시된 Facebook의 RoBERTa 모델을 기반으로 합니다. 2.5TB의 필터링된 CommonCrawl 데이터에서 훈련된 대규모 다국어 언어 모델입니다.\nRoBERTa 모델은 RoBERTa: A Robustly Optimized BERT Pretraining Approach에서 제안되었습니다. 2018년에 출시된 Google의 BERT 모델을 기반으로 합니다. BERT를 기반으로 하며 주요 하이퍼파라미터를 수정하여 다음 문장 사전 훈련 목표를 제거하고 훨씬 더 큰 미니 배치 및 학습 속도로 훈련합니다."
  },
  {
    "objectID": "dictionary/terms/xlm-roberta.html#참조",
    "href": "dictionary/terms/xlm-roberta.html#참조",
    "title": "XLM-RoBERTa",
    "section": "참조",
    "text": "참조\n\nhttps://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment\nhttps://github.com/facebookresearch/fairseq/tree/main/examples/xlmr"
  },
  {
    "objectID": "dictionary/terms/directory_1.html",
    "href": "dictionary/terms/directory_1.html",
    "title": "directory",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/code_snippet.html",
    "href": "dictionary/terms/code_snippet.html",
    "title": "code snippet",
    "section": "",
    "text": "예시 코드는 일반적으로 소프트웨어 개발에서 사용되는 용어로, 작은 부분의 코드 조각 또는 프로그래밍 코드의 짧은 일부분을 가리킵니다.\n이러한 코드 조각은 주로 특정 작업을 수행하기 위해 사용되며, 보통 몇 줄에서 수십 줄 정도의 길이일 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/code_snippet.html#참조",
    "href": "dictionary/terms/code_snippet.html#참조",
    "title": "code snippet",
    "section": "참조",
    "text": "참조\n\n[code snippet의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/training.html",
    "href": "dictionary/terms/training.html",
    "title": "training",
    "section": "",
    "text": "딥러닝에서 훈련(training)은 모델을 데이터에 훈련시키는 과정을 의미합니다. 이 과정은 모델이 데이터에서 패턴을 이해하고 문제를 해결하기 위한 모델을 조정하는 것을 포함합니다.\n훈련은 입력 데이터를 모델에 주입하고, 모델은 예측을 수행한 후 손실 함수를 사용하여 오차를 계산하고 역전파(backpropagation)를 통해 파라미터를 업데이트합니다. 이 과정은 여러 에폭(epochs) 동안 반복됩니다."
  },
  {
    "objectID": "dictionary/terms/training.html#참조",
    "href": "dictionary/terms/training.html#참조",
    "title": "training",
    "section": "참조",
    "text": "참조\n\n[딥러닝 training의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/pre_train.html",
    "href": "dictionary/terms/pre_train.html",
    "title": "pre-train",
    "section": "",
    "text": "사전 학습은 딥러닝 모델을 처음부터 학습시키는 것이 아니라, 미리 학습된 모델의 가중치(weights)와 특성을 활용하여 초기 가중치를 설정하는 기술입니다.\n미리 학습된 모델은 대규모 데이터로 학습되었으며, 일반적인 특징과 패턴을 잘 파악하고 있습니다. 따라서 새로운 작업을 위해 이러한 모델을 사용하면 초기 가중치가 우수하며, 빠른 수렴과 높은 성능을 제공할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/pre_train.html#참조",
    "href": "dictionary/terms/pre_train.html#참조",
    "title": "pre-train",
    "section": "참조",
    "text": "참조\n\n[딥러닝 pre train의 의미]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/shape.html",
    "href": "dictionary/terms/shape.html",
    "title": "shape",
    "section": "",
    "text": "신경망 네트워크 행렬의 크기"
  },
  {
    "objectID": "dictionary/terms/shape.html#참조",
    "href": "dictionary/terms/shape.html#참조",
    "title": "shape",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/terms/epoch.html",
    "href": "dictionary/terms/epoch.html",
    "title": "epoch",
    "section": "",
    "text": "딥러닝에서 에폭은 모델의 훈련량을 나타내는 단위로, 1 에폭은 학습 데이터 전체를 한 번 순회하는 것을 의미합니다.\n모델 훈련 시 학습 데이터는 배치라고 부르는 데이터 묶음으로 나뉘어 모델에 전달됩니다. 따라서 배치 크기에 따라서 1 에폭에 모델이 훈련되는 횟수가 달라집니다.\n예를 들어 학습 데이터의 총 샘플 수가 100이고 배치 크기가 10이라면 1 에폭당 10회의 훈련이 진행됩니다."
  },
  {
    "objectID": "dictionary/terms/epoch.html#참조",
    "href": "dictionary/terms/epoch.html#참조",
    "title": "epoch",
    "section": "참조",
    "text": "참조\n\nhttps://machinelearningmastery.com/difference-between-a-batch-and-an-epoch"
  },
  {
    "objectID": "dictionary/terms/section.html",
    "href": "dictionary/terms/section.html",
    "title": "section",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/submodule.html",
    "href": "dictionary/terms/submodule.html",
    "title": "submodule",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/tokenizer.html",
    "href": "dictionary/terms/tokenizer.html",
    "title": "tokenizer",
    "section": "",
    "text": "토크나이저는 자연어 처리(Natural Language Processing, NLP) 분야에서 텍스트를 작은 단위로 나누는 도구 또는 프로세스를 가리킵니다. 이 작은 단위는 토큰(Token)이라고 불리며, 일반적으로 단어, 문장 부호, 혹은 하나의 글자와 같은 작은 텍스트 조각을 말합니다.\n토크나이저의 주요 목적은 텍스트를 기계 학습 알고리즘, 딥러닝 모델 또는 다른 자연어 처리 작업에 입력으로 사용할 수 있는 형식으로 변환하는 것입니다. 이렇게 텍스트를 토큰으로 분할하면 기계는 더 쉽게 텍스트를 이해하고 처리할 수 있습니다."
  },
  {
    "objectID": "dictionary/terms/tokenizer.html#참조",
    "href": "dictionary/terms/tokenizer.html#참조",
    "title": "tokenizer",
    "section": "참조",
    "text": "참조\n\n[tokenizer]. https://chat.openai.com/"
  },
  {
    "objectID": "dictionary/terms/half_precision.html",
    "href": "dictionary/terms/half_precision.html",
    "title": "half precision",
    "section": "",
    "text": "설명을 추가해주세요."
  },
  {
    "objectID": "dictionary/terms/capsule_netorwk.html",
    "href": "dictionary/terms/capsule_netorwk.html",
    "title": "capsule network",
    "section": "",
    "text": "업로드 예정"
  },
  {
    "objectID": "dictionary/terms/capsule_netorwk.html#참조",
    "href": "dictionary/terms/capsule_netorwk.html#참조",
    "title": "capsule network",
    "section": "참조",
    "text": "참조"
  },
  {
    "objectID": "dictionary/glossary.html",
    "href": "dictionary/glossary.html",
    "title": "용어 사전",
    "section": "",
    "text": "딥러닝관련 프로젝트를 번역하는 과정에서 용어를 일관성 있게 번역하기 위해서 사전을 만들고 있습니다. 딥러닝 문서 번역에 관심있는 분들 모두 참가가 가능하니 관심있는 분들의 많은 참여 부탁드립니다.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nJansen-Shannon divergence\n\n\n\n\n\n잰슨-섀넌 다이버전스\n\n\n\n\n\n\nhttps://nn.labml.ai/gan/wasserstein/index.html\n\n\n\n\n\n\n\n\nL2 norm\n\n\n\n\n\nL2 노름\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nXLM-RoBERTa\n\n\n\n\n\nXLM-RoBERTa\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/model_doc/xlm-roberta\n\n\n\n\n\n\n\n\narchitecture\n\n\n\n\n\n아키텍처\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nargumentation\n\n\n\n\n\n증강\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\narguments\n\n\n\n\n\n인수\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nattention mask\n\n\n\n\n\n어텐션 마스크\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nautomatic speech recognition\n\n\n\n\n\n자동음성인식\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nautoregressive model\n\n\n\n\n\n자기회귀 모델\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nbackward\n\n\n\n\n\n역방향\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nbatch\n\n\n\n\n\n배치\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nbranch\n\n\n\n\n\n브랜치\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ncapsule network\n\n\n\n\n\n캡슐 네트워크\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\ncausal inference\n\n\n\n\n\n인과추론\n\n\n\n\n\n\nnull\n\n\n\n\n\n\n\n\ncausal language model\n\n\n\n\n\n인과적 언어 모델링\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncheckpoint\n\n\n\n\n\n체크포인트\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nchunk\n\n\n\n\n\n묶음\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nclustering\n\n\n\n\n\n군집화\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncode snippet\n\n\n\n\n\n예시 코드\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nconfiguration\n\n\n\n\n\n설정값\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nconvolution\n\n\n\n\n\n합성곱\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nconvolution\n\n\n\n\n\n컨볼루션\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ncrob\n\n\n\n\n\n자르기\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncustom\n\n\n\n\n\n사용자정의\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ncustom\n\n\n\n\n\n커스텀\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndata collator\n\n\n\n\n\n데이터 콜레이터\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndataset\n\n\n\n\n\n데이터 세트\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndenoise\n\n\n\n\n\n디노이즈\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndepth-wise convolution\n\n\n\n\n\ndepth-wise 컨볼루션\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\ndimensions ordering\n\n\n\n\n\n차원순서(dimensions ordering)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndirectory\n\n\n\n\n\n디렉터리\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndirectory\n\n\n\n\n\n디렉토리\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ndiscriminator\n\n\n\n\n\n판별자\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\ndistributed training\n\n\n\n\n\n분산 학습\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ndown stream\n\n\n\n\n\n다운 스트림\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nearth mover distance\n\n\n\n\n\nearth mover 거리\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nentity\n\n\n\n\n\n개체\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nepoch\n\n\n\n\n\n에폭\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nevaluation method\n\n\n\n\n\n평가 방법\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfeature extraction\n\n\n\n\n\n특성 추출\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfeature matrix\n\n\n\n\n\n특성 행렬\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfine tunning\n\n\n\n\n\n파인튜닝\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nfine tunning\n\n\n\n\n\n미세 조정\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ngenerator\n\n\n\n\n\n생성자\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\ngradient\n\n\n\n\n\n그래디언트\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nhalf precision\n\n\n\n\n\n반정밀도(half precision)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nhidden state\n\n\n\n\n\n은닉 상태\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nhyperparameter\n\n\n\n\n\n하이퍼파라미터\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ninference\n\n\n\n\n\n추론\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ninpainting\n\n\n\n\n\n인페인팅\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ninstance\n\n\n\n\n\n인스턴스\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\niteration\n\n\n\n\n\n반복\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nlabel smoothing function\n\n\n\n\n\n라벨 스무딩 함수\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n학습\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nlesson\n\n\n\n\n\n단원\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nlicense\n\n\n\n\n\n라이선스\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nload\n\n\n\n\n\n가져오다\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nlogging\n\n\n\n\n\n로깅\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nlogit\n\n\n\n\n\n로짓\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nloss landscape\n\n\n\n\n\n손실 공간\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nmethod\n\n\n\n\n\n매소드\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nmixed precision\n\n\n\n\n\n혼합 정밀도(mixed precision)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nmodel\n\n\n\n\n\n모델\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nmulti-image batches\n\n\n\n\n\n다중 이미지 배치(multi-image batches)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\noptimizer\n\n\n\n\n\n옵티마이저\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nover fitting\n\n\n\n\n\n과적합\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\npading\n\n\n\n\n\n패딩\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\npoint-wise convolution\n\n\n\n\n\npoint-wise 컨볼루션\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\npooling\n\n\n\n\n\n풀링\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\npre-train\n\n\n\n\n\n사전학습\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nprior-preserving\n\n\n\n\n\n사전 보존(prior-preserving)\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nprompt\n\n\n\n\n\n프롬프트\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\npush\n\n\n\n\n\n푸시\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nrepository\n\n\n\n\n\n리포지토리\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nreproducibility\n\n\n\n\n\n재현성\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nresume\n\n\n\n\n\n재개\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nsection\n\n\n\n\n\n섹션\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nseperator\n\n\n\n\n\n분할 토큰\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nsequence\n\n\n\n\n\n시퀀스\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nshape\n\n\n\n\n\n크기\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nsilent error\n\n\n\n\n\n조용한 오류\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nspectrogram\n\n\n\n\n\n스펙토그램\n\n\n\n\n\n\nhttps://huggingface.co/learn/audio-course/chapter0/introduction\n\n\n\n\n\n\n\n\nspeech enhancement\n\n\n\n\n\n음성 향상\n\n\n\n\n\n\nhttps://huggingface.co/learn/audio-course/\n\n\n\n\n\n\n\n\nstep\n\n\n\n\n\n스텝\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\nstrategy profile\n\n\n\n\n\n전략 프로필\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nsubmodule\n\n\n\n\n\n서브모듈\n\n\n\n\n\n\nhttps://huggingface.co/docs/diffusers/index\n\n\n\n\n\n\n\n\ntoken\n\n\n\n\n\n토큰\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ntokenizer\n\n\n\n\n\n토크나이저\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\ntraining\n\n\n\n\n\n훈련\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/index\n\n\n\n\n\n\n\n\nvalidation\n\n\n\n\n\n검증\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\n\n\nvocoder\n\n\n\n\n\n보코더\n\n\n\n\n\n\nhuggingface_audio\n\n\nhttps://huggingface.co/learn/audio-course/chapter0/introduction\n\n\n\n\n\n\n\n\nweight decay\n\n\n\n\n\n가중치 감쇠\n\n\n\n\n\n\nhttps://nn.labml.ai\n\n\n\n\n\n\nNo matching items"
  }
]